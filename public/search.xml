<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[es集群节点出现overhead脱机的问题]]></title>
    <url>%2F2020%2F03%2F12%2F38-es%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E5%87%BA%E7%8E%B0overhead%E8%84%B1%E6%9C%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[elasticsearch 日志提示 overhead, 导致集群出现问题 问题说明 elasticsearch 日志提示 overhead123456[2020-03-12T14:38:03,565][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][old][3008939][256208] duration [18.4s], collections [1]/[18.9s], total [18.4s]/[5.7h], memory [7.3gb]-&gt;[7.3gb]/[7.9gb], all_pools &#123;[young] [17.5mb]-&gt;[3.3mb]/[532.5mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[66.5mb]&#125;&#123;[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]&#125;[2020-03-12T14:38:03,593][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][3008939] overhead, spent [18.4s] collecting in the last [18.9s][2020-03-12T14:37:44,632][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][old][3008938][256207] duration [24.8s], collections [1]/[25.5s], total [24.8s]/[5.7h], memory [7.3gb]-&gt;[7.3gb]/[7.9gb], all_pools &#123;[young] [8.5mb]-&gt;[17.5mb]/[532.5mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[66.5mb]&#125;&#123;[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]&#125;[2020-03-12T14:37:44,632][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][3008938] overhead, spent [24.8s] collecting in the last [25.5s] 查看elasticsearch 配置 heap size 是8G ES 内存使用和GC指标——默认情况下，主节点每30秒会去检查其他节点的状态，如果任何节点的垃圾回收时间超过30秒（Garbage collection duration），则会导致主节点任务该节点脱离集群。 设置过大的heap会导致GC时间过长，这些长时间的停顿（stop-the-world）会让集群错误的认为该节点已经脱离。 所以通过增加ping_timeout的时间，和增加ping_retries的次数来防止节点错误的脱离集群，可以使节点有充足的时间进行full GC。 问题解决 这里将默认的超时时间增加, 增加重试次数, 增加间隔时间1234#超时时间设为5分钟，超过6次心跳没有回应，则认为该节点脱离master，每隔60s发送一次心跳。 discovery.zen.fd.ping_timeout: 300s discovery.zen.fd.ping_retries: 6 discovery.zen.fd.ping_interval: 60s gc 垃圾回收算法 摘自原文: https://www.jianshu.com/p/1f450826f62e 标记-清除 算法(Mark Sweep)该算法很简单，使用通过可达性分析分析方法标记出垃圾，然后直接回收掉垃圾区域。它的一个显著问题是一段时间后，内存会出现大量碎片，导致虽然碎片总和很大，但无法满足一个大对象的内存申请，从而导致 OOM，而过多的内存碎片（需要类似链表的数据结构维护），也会导致标记和清除的操作成本高，效率低下，如下图所示： 复制算法(Copying)有人提出了复制算法。它将可用内存一分为二，每次只用一块，当这一块内存不够用时，便触发 GC，将当前存活对象复制(Copy)到另一块上，以此往复。这种算法高效的原因在于分配内存时只需要将指针后移，不需要维护链表等。但它最大的问题是对内存的浪费，使用率只有 50% 标记-整理算法(Mark Compact)该算法解决了第1中算法的内存碎片问题，它会在回收阶段将所有内存做整理 分代收集算法(Generation Collection)既然大部分 Java 对象是朝生夕死的，那么我们将内存按照 Java 生存时间分为 新生代(Young) 和 老年代(Old)，前者存放短命僧，后者存放长寿佛，当然长寿佛也是由短命僧升级上来的。然后针对两者可以采用不同的回收算法，比如对于新生代采用复制算法会比较高效，而对老年代可以采用标记-清除或者标记-整理算法。这种算法也是最常用的。JVM Heap 分代后的划分一般如下所示，新生代一般会分为 Eden、Survivor0、Survivor1区，便于使用复制算法。 将内存分代后的 GC 过程一般类似下图所示： 1 对象一般都是先在 Eden区创建2 当Eden区满，触发 Young GC，此时将 Eden中还存活的对象复制到 S0中，并清空 Eden区后继续为新的对象分配内存3 当Eden区再次满后，触发又一次的 Young GC，此时会将 Eden和S0中存活的对象复制到 S1中，然后清空Eden和S0后继续为新的对象分配内存4 每经过一次 Young GC，存活下来的对象都会将自己存活次数加1，当达到一定次数后，会随着一次 Young GC 晋升到 Old区5 Old区也会在合适的时机进行自己的 GC elasticsearch gc说明 Elasticsearch 默认的 GC 配置是CMS GC ，其 Young 区用 ParNew，Old 区用CMS，大家可以在 config/jvm.options中看到如下的配置： 1234## GC configuration-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSInitiatingOccupancyOnly 何时进行回收1231 Young 区的GC 都是在 Eden 区满时触发2 Serial Old 和 Parallel Old 在 Old 区是在 Young GC 时预测Old 区是否可以为 young 区 promote 到 old 区 的 object 分配空间，如果不可用则触发 Old GC。这个也可以理解为是 Old区满时。3 CMS GC 是在 Old 区大小超过一定比例后触发，而不是 Old 区满。这个原因在于 CMS GC 是并发的算法，也就是说在 GC 线程收集垃圾的时候，用户线程也在运行，因此需要预留一些 Heap 空间给用户线程使用，防止由于无法分配空间而导致 Full GC 发生。 gc 日志说明123[2020-03-12T14:38:03,565][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][old][3008939][256208] duration [18.4s], collections [1]/[18.9s], total [18.4s]/[5.7h], memory [7.3gb]-&gt;[7.3gb]/[7.9gb], all_pools &#123;[young] [17.5mb]-&gt;[3.3mb]/[532.5mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[66.5mb]&#125;&#123;[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]&#125;[2020-03-12T14:38:03,593][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][3008939] overhead, spent [18.4s] collecting in the last [18.9s] 本次是old gc, 这是第3008939次GC检查, 从java启动至今这是第256208次 gc 共花18.4s, [从上次检查至今共发生一次gc][从上次检查至今已经过去18.9s],[本次gc18.4s]/[从 JVM 启动至今发生的 GC 总耗时为5.7h], [ GC 前 Heap memory 空间]-&gt;[GC 后 Heap memory 空间]/[Heap memory 总空间] {[young 区][GC 前 Memory ]-&gt;[GC后 Memory]/[young区 Memory 总大小] } {[survivor 区][GC 前 Memory ]-&gt;[GC后 Memory]/[survivor区 Memory 总大小] }{[old 区][GC 前 Memory ]-&gt;[GC后 Memory]/[old区 Memory 总大小] } dc2da94b55eddf72936367b2aecfa39fc9b4aca8]]></content>
      <categories>
        <category>elk</category>
        <category>elasticsearch5</category>
      </categories>
      <tags>
        <tag>elasticsearch5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac一些常用命令]]></title>
    <url>%2F2020%2F03%2F10%2F37-mac%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[记录一些常用的mac工具和方法 1. Item2 1.1 同时按住 option(alt) 键，可以以列选中，类似于 sublime3中 按住 alt的列选中 。Command + option(alt) 1.2 剪贴板历史记录Command + Shift + h 1.3 将文本内容复制到剪切板pbcopy &lt; a.txt]]></content>
      <categories>
        <category>技术文档</category>
        <category>mac</category>
        <category>item2</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos单网卡配置多ip的几种方法]]></title>
    <url>%2F2020%2F02%2F27%2F36-centos%E5%8D%95%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE%E5%A4%9Aip%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[centos单网卡配置多ip的几种方法 方法一 新建IP别名 临时设置, 不需要重启 12ifconfig enp0s3:1 172.16.53.109/24ifconfig enp0s3:1 down 配置文件设置, 需要重启 12345678910#cat ifcfg-enp0s3:1DEVICE=enp0s3IPADDR=172.16.53.109NETMASK=255.255.255.0# 重启网络service network restart# 查看(ifconfig 也可以查看)ip a 或ifconfig 方法二 临时设置, 不需要重启1ip addr add 172.16.53.110/24 dev enp0s3 label enp0s3:2 方法三 临时设置, 不需要重启1ifconfig enp0s3:3 172.16.53.111 netmask 255.255.255.0 方法四 同一个配置文件设置, 需要重启。IP地址没有别名不好进行管理。1234567891011121314#cat ifcfg-enp0s3DEVICE=enp0s3IPADDR=172.16.53.106IPADDR1=172.16.53.112IPADDR2=172.16.53.113PREFIX=24PREFIX1=24PREFIX2=24# 重启网络service network restart# 查看(ifconfig 不可以查看)ip a 注:这里奇怪的是, 实际配置中,出现个别ip使用方法二,三时仅部分内网可以联通,例如10.10.76.1 通过方法二配置, 从10.10.76.2上可以ping通, 但是从10.10.53.1上无法ping通(10.10.53.1和10.10.76.2是可以ping通)但是通过方法四配置就正常… 目前没有找到原因… 或与公司路由器有关]]></content>
      <categories>
        <category>技术文档</category>
        <category>linux</category>
        <category>网卡</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raid1盘数据迁移]]></title>
    <url>%2F2020%2F02%2F27%2F35-raid1%E7%9B%98%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[dell PowerEdge 1950 服务器两块盘做raid1的linux操作系统, 开机后无限重启的一次数据迁移 考虑到raid1数据是互为备份,直接取一块盘应该能够拿到所有数据 首先对dell PowerEdge 1950 服务器 开机, 在提示ctrl +c的页面上进入sas页面, 进入选中磁盘后回车, 然后选择 SAS Topology页面, 可以看到是两块盘做的raid1 raid1 信息确认完毕 关闭1950服务器, 取下其中一块盘, 这里看到硬盘是sata盘 考虑到该盘不确定是否支持热插拔, 这里是将sata盘放入usb盘接到某台Linux服务器, 然后挂载, 挂载注意fdisk -l 看下具体分区, 我这里是/dev/sdb3 mount /data /dev/sdb3 进入/data, 就会看到raid1硬盘中保留的所有数据]]></content>
      <categories>
        <category>技术文档</category>
        <category>raid</category>
      </categories>
      <tags>
        <tag>raid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s一些命令总结]]></title>
    <url>%2F2019%2F12%2F05%2F34-k8s%E4%B8%80%E4%BA%9B%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[记录一些kubectl命令 kubectl命令表 常用命令12345678# 让内网可以访问 k8s proxy(k8smaster是:172.16.1.111kubectl proxy --address='172.16.1.111' -p 10000 --accept-hosts='^172.*$'# 查看api类型kubectl api-versions # 让master也运行pod（默认master不运行pod,单机会用到）kubectl taint nodes --all node-role.kubernetes.io/master- scale 使用123# 通过将rc的副本数重新设置为0后，再将副本数设置为2，达到重启nginx的效果。kubectl scale deployment bq-front1 --replicas=0 -n webkubectl scale deployment bq-front1 --replicas=2 -n web metrics 相关123456# 查看node 资源kubectl top nodes# 查看pods 资源kubectl top pods -n php-dev# 获取metrics接口所有数据kubectl get --raw /metrics 根据版本缩放12345678910#查看Deployment的变更信息（以下信息得以保存，是创建时候加的“--record”这个选项起的作用）：kubectl rollout history deployment/bq-nginx-php7 -n webkubectl rollout undo deployment/bq-nginx-php7 # 回退到上一版本kubectl rollout undo deployment/bq-nginx-php7 --to-revision=2 # 回退到指定版本kubectl describe deployments/bq-nginx-php7 -n web #查询详细信息，获取升级进度kubectl rollout pause deployment/bq-nginx-php7 -n web #暂停升级kubectl rollout resume deployment/bq-nginx-php7 -n web #继续升级kubectl rollout undo deployment/bq-nginx-php7 -n web #升级回滚kubectl scale deployment bq-nginx-php7 --replicas 2 -n web #弹性伸缩Pod数量]]></content>
      <categories>
        <category>k8s</category>
        <category>kubectl</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubectl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k3s安装配置]]></title>
    <url>%2F2019%2F12%2F03%2F29-k3s%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[体验轻量级k8s集群,适用于低配个人开发测试使用 k3s, 5 less than k8s 详情参考官方: k3s github地址 准备 1 selinux 关闭 123456getenforce# 本次关闭setenforce 0# 重启后关闭sed -i '/SELINUX=enforcing/s/enforcing/disabled/' /etc/sysconfig/selinux 2 关闭swap(可选) 1234# 本次关闭swapoff on# 重启后关闭sed -i '/swap/s@^/@#/@' /etc/fstab 3 关闭firewalld(必须) 12systemctl stop firewalld.servicesystemctl disable firewalld.service 4 在内核3.10,4.16,5.2,5.3 都正常运行 Step 1: 安装K3S集群1234567891011121314151617181920212223# 下载k3s 二进制文件打开各版本点击详情可以查询k3s版本对应的k8s版本(https://github.com/rancher/k3s/releases)wget https://github.com/rancher/k3s/releases/download/v1.0.0/k3sk3s v1.0.0 -&gt; k8s1.16.3# https://github.com/rancher/k3s/tagsk3s v0.9.0 -&gt; k8s1.15.4k3s v0.10.0 -&gt; k8s1.16.2我这里下载最新的k3s v1.0.0, 但是由于metrics-server好像对k8s1.16.3最新有点问题, 还是先等待metrics-server更新把测试了k3s v0.10.0 测试了下, 但遗憾的是默认好像没有安装metrics-server...mv k3s /usr/local/bin/k3schmod +x /usr/local/bin/k3s#k3s --versionk3s version v1.0.0 (18bd921c)# 下载pause镜像(这里举1,其他国内地址参考官方)docker pull registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1docker tag registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1 k8s.gcr.io/pause:3.1# 验证一下docker images | grep "k8s.gcr.io/pause" Step 2: 安装k3s server12345678910111213141516# centos官方安装curl -sfL https://get.k3s.io | sh -# 至此server已经安装完了,但由于k8s默认是用Containerd, 并非docker, 所以需要手工修改配置(当然如果你熟悉ctr 操作Containerd也没问题)# 修改ExecStart内容# 1: --docker 表示k3s server使用docker引擎# 2: --no-deploy traefik 表示不安装traefikvim /etc/systemd/system/multi-user.target.wants/k3s.serviceExecStart=/usr/local/bin/k3s server --docker --no-deploy traefik# 启动服务systemctl daemon-reloadservice k3s restart# 验证k3s kubectl get node 想去掉k3s命令? kubectl命令管理k3s 1234567# 简单做一个aliasalias kubectl='k3s kubectl'# 或者rm -rf ~/.kubemkdir -p ~/.kubecp /etc/rancher/k3s/k3s.yaml ~/.kube/config Step 3: 客户端安装参考官方文档 安装和配置选项 1234# 同样下载二进制包wget https://github.com/rancher/k3s/releases/download/v1.0.0/k3smv k3s /usr/local/bin/k3schmod +x /usr/local/bin/k3s 加入到server有两种 手动加入 (其实上面我们已经拉取了image, 并且tag成官方地址了,所以这里也可以不用指定) 12nohup k3s agent --docker --pause-image registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1 --server https://k3s-server:6443 --token $&#123;NODE_TOKEN&#125; &amp;nohup k3s agent --docker --server https://k3s-server:6443 --token $&#123;NODE_TOKEN&#125; &amp; 脚本加入 12345curl -sfL https://get.k3s.io | K3S_URL=https://k3s-server:6443 K3S_TOKEN=$&#123;NODE_TOKEN&#125; INSTALL_K3S_EXEC="agent --docker --pause-image registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1" sh -s -curl -sfL https://get.k3s.io | K3S_URL=https://k3s-server:6443 K3S_TOKEN=$&#123;NODE_TOKEN&#125; INSTALL_K3S_EXEC="agent --docker" sh -s -# ps aux|grep k3s/usr/local/bin/k3s agent --docker --pause-image registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1 当然如下差别不大, 都是会启动一个k3s的进程 rancher import1curl --insecure -sfL https://rancher-dev.xxx.com/v3/import/x8jc277zmjkxjgcmc9f67pzn9f7ffsjpszlv9dxc79vhmndqcms4nr.yaml | k3s kubectl apply -f - 卸载1sh /usr/local/bin/k3s-uninstall.sh]]></content>
      <categories>
        <category>技术文档</category>
        <category>k3s</category>
      </categories>
      <tags>
        <tag>k3s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k3s1.16部署nginx+php5.2.17]]></title>
    <url>%2F2019%2F12%2F03%2F33-k3s1.16%E9%83%A8%E7%BD%B2nginx%2Bphp5.2.17%2F</url>
    <content type="text"><![CDATA[老项目是用php5.2.17的,自己编译打包镜像简单部署 开始部署 准备dockerfile Dockerfile 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798FROM centos:6.9MAINTAINER zhangzw zhangzw@boqii.comENV PHP_DIR /usr/local/phpENV WORK_DIR_tar /usr/loca/src/ENV PHP_VERSION 5.2.17ENV PHP_EXT_CURL curl-7.20.0# php 及扩展 包,包括以下内容# php-5.2.17-patch-fpm.tar.gz curl-7.20.0.tar.gz freetype-2.4.0.tar.gz ImageMagick-6.9.0-4.tar.gz imagick-3.0.1.tgz zendopcache-7.0.5.tgz phpredis-2.2.2.zip # php-fpm.conf php.ini copy tar $&#123;WORK_DIR_tar&#125;run yum install -y wget \ &amp;&amp; wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo \ &amp;&amp; rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 \ &amp;&amp; yum install -y epel-release \ &amp;&amp; yum install -y freetype freetype-devel gcc make cmake ncurses-devel gcc-c++ autoconf automake zlib-devel dos2unix nc lrzsz openssl-devel pcre-devel libxml2 libxml2-devel libcurl libcurl-devel libpng-devel bzip2-devel libjpeg libjpeg-turbo-devel libmcrypt-devel mhash-devel mysql-devel libtool-ltdl libtool-ltdl-devel git bzip2-devel git supervisor autoconf automake xz unzip \ &amp;&amp; yum clean all &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; ls *gz|xargs -i tar -xf &#123;&#125; \ &amp;&amp; cd $&#123;PHP_EXT_CURL&#125; \ &amp;&amp; ./configure --prefix=/usr/local/curl \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd php-$&#123;PHP_VERSION&#125; \ &amp;&amp; ln -s /usr/lib64/libpng.so /usr/lib/ \ &amp;&amp; ln -s /usr/lib64/libjpeg.so /usr/lib/ \ &amp;&amp; ln -s /usr/lib64/mysql/libmysqlclient.so.16.0.0 /usr/lib/libmysqlclient.so \ &amp;&amp; ./configure \ --prefix=$&#123;PHP_DIR&#125; \ --with-config-file-path=$&#123;PHP_DIR&#125;/etc \ --with-mysql \ --with-mysqli \ --with-openssl \ --enable-fastcgi \ --enable-fpm \ --enable-mbstring \ --enable-bcmath \ --with-freetype-dir \ --with-jpeg-dir \ --with-png-dir \ --with-zlib-dir \ --with-libxml-dir=/usr \ --enable-xml \ --with-mhash \ --with-mcrypt \ --enable-pcntl \ --enable-sockets \ --with-bz2 \ --with-curl=/usr/local/curl \ --with-curlwrappers \ --enable-mbregex \ --with-gd \ --enable-gd-native-ttf \ --enable-zip \ --enable-soap \ --with-iconv \ --enable-pdo \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd ImageMagick-6.9.0-4 \ &amp;&amp; ./configure --prefix=/usr/local/imagemagick \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd imagick-3.0.1 \ &amp;&amp; ln -s /usr/local/imagemagick/include/ImageMagick-6 /usr/local/imagemagick/include/ImageMagick \ &amp;&amp; $&#123;PHP_DIR&#125;/bin/phpize \ &amp;&amp; ./configure --with-php-config=$&#123;PHP_DIR&#125;/bin/php-config --with-imagick=/usr/local/imagemagick \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; unzip phpredis-2.2.2.zip \ &amp;&amp; cd phpredis-2.2.2 \ &amp;&amp; $&#123;PHP_DIR&#125;/bin/phpize \ &amp;&amp; ./configure --with-php-config=$&#123;PHP_DIR&#125;/bin/php-config \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd zendopcache-7.0.5 \ &amp;&amp; $&#123;PHP_DIR&#125;/bin/phpize \ &amp;&amp; ./configure --with-php-config=$&#123;PHP_DIR&#125;/bin/php-config \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; groupadd -r www \ &amp;&amp; useradd -M -s /sbin/nologin -r -g www www \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; \cp -r $&#123;WORK_DIR_tar&#125;/php-fpm.conf $&#123;PHP_DIR&#125;/etc/ \ &amp;&amp; \cp -r $&#123;WORK_DIR_tar&#125;/php.ini $&#123;PHP_DIR&#125;/etc/ \ &amp;&amp; rm -rf $&#123;WORK_DIR_tar&#125;copy supervisord-fpm.conf /etc/supervisord.confcopy start.sh /root/start.shENTRYPOINT ["/bin/sh", "/root/start.sh"] build 打包 123# 配置自己的私有仓库地址docker build -t xxx.com/centos-php:5.2.17 .docker push xxx.com/centos-php:5.2.17 可参考k3s安装教程: k3s安装配置 官方教程 在k3s中启动(这里本地挂载方式,单节点) nginx.conf 部分配置 1234567891011121314151617181920212223server &#123; listen 80 default_server; server_name _; access_log /webwww/nginx_logs/test_access.log main; error_log /webwww/nginx_logs/test_error.log debug; root /webwww/test; location = /50x.html &#123; root html; &#125; location / &#123; index index.php index.html index.htm; &#125; location ~ \.php$ &#123; fastcgi_pass php-fpm-dev:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTP_HOST $server_name; include fastcgi_params; &#125;&#125; nginx部署 nginx/php-nginx-dev.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758---apiVersion: apps/v1kind: Deploymentmetadata: name: php-nginx-dev namespace: php-devspec: replicas: 1 selector: matchLabels: app: php-nginx-dev template: metadata: labels: app: php-nginx-dev spec: containers: - name: php-nginx-dev image: hub.boqii.com/bq/nginx:1.15.12 ports: - containerPort: 80 name: nginx-80 protocol: TCP resources: requests: cpu: "10m" limits: cpu: "500m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: nginx-cfg-dev mountPath: "/etc/nginx/nginx.conf" volumes: - name: nginx-www-dev hostPath: path: /data/k8s-container/php-5.2.17/webwww-data - name: nginx-cfg-dev hostPath: path: /data/k8s-container/php-5.2.17/nginx/nginx.conf---kind: ServiceapiVersion: v1metadata: labels: app: php-nginx-dev name: php-nginx-dev-service namespace: php-devspec: type: NodePort ports: - name: nginx-80 port: 80 targetPort: 80 nodePort: 32001 protocol: TCP selector: app: php-nginx-dev fpm 部署配置 php-fpm/php-fpm-dev.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061---apiVersion: apps/v1kind: Deploymentmetadata: name: php-fpm-dev namespace: php-devspec: replicas: 1 selector: matchLabels: app: php-fpm-dev template: metadata: labels: app: php-fpm-dev spec: containers: - name: php-fpm-dev image: hub.boqii.com/bq/centos-php:5.2.17 ports: - containerPort: 9000 name: fpm-9000 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "1500m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: php-cfg-dev mountPath: "/usr/local/php/etc/php.ini" - name: fpm-cfg-dev mountPath: "/usr/local/php/etc/php-fpm.conf" volumes: - name: nginx-www-dev hostPath: path: /data/k8s-container/php-5.2.17/webwww-data - name: php-cfg-dev hostPath: path: /data/k8s-container/php-5.2.17/php-fpm/php.ini - name: fpm-cfg-dev hostPath: path: /data/k8s-container/php-5.2.17/php-fpm/php-fpm.conf---apiVersion: v1kind: Servicemetadata: name: php-fpm-dev namespace: php-devspec: clusterIP: None selector: app: php-fpm-dev ports: - name: fpm-9000 port: 9000--- 部署命令123# 先启动fpm,否则nginx会报错找不到 php-fpm-devkubectl apply -f php-fpm/php-fpm-dev.ymlkubectl apply -f nginx/php-nginx-dev.yml]]></content>
      <categories>
        <category>技术文档</category>
        <category>k3s</category>
        <category>lnmp</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>k8s</tag>
        <tag>k3s</tag>
        <tag>php5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php错误502问题总结]]></title>
    <url>%2F2019%2F12%2F02%2F32-php%E9%94%99%E8%AF%AF502%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
      <categories>
        <category>技术文档</category>
        <category>php</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[systemd一些命令]]></title>
    <url>%2F2019%2F11%2F26%2F31-systemd%E4%B8%80%E4%BA%9B%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[记录一些需要systemd命令 systemd命令 查看服务启动配置: systemctl cat k3s 查看开机启动的服务列表：systemctl list-unit-files|grep enabled 查看启动失败的服务列表：systemctl --failed 重启服务：systemctl restart firewalld.service 显示状态：systemctl status firewalld.service 开机启用服务：systemctl enable firewalld.service 开机禁用服务：systemctl disable firewalld.service 查看开机启动：systemctl is-enabled firewalld.service]]></content>
      <categories>
        <category>linux</category>
        <category>systemd</category>
      </categories>
      <tags>
        <tag>systemd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[29-k3s集群部署项目报挂载nfs错误]]></title>
    <url>%2F2019%2F11%2F25%2F29-k3s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E6%8A%A5%E6%8C%82%E8%BD%BDnfs%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[k3s集群部署项目报挂载nfs错误]]></title>
    <url>%2F2019%2F11%2F25%2F30-k3s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E6%8A%A5%E6%8C%82%E8%BD%BDnfs%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[体验轻量级k8s集群遇到一些nfs问题 部署服务器查看describe信息如下:1234567Mounting command: systemd-runMounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/369daaef-1e90-446b-92ce-3d562f94b429/volumes/kubernetes.io~nfs/pvc-f462c606-5796-4c48-8928-7822f3fa0605 --scope -- mount -t nfs 172.16.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-es520-2-dev-1-pvc-f462c606-5796-4c48-8928-7822f3fa0605 /var/lib/kubelet/pods/369daaef-1e90-446b-92ce-3d562f94b429/volumes/kubernetes.io~nfs/pvc-f462c606-5796-4c48-8928-7822f3fa0605Output: Running scope as unit run-14829.scope.mount: 文件系统类型错误、选项错误、172.16.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-es520-2-dev-1-pvc-f462c606-5796-4c48-8928-7822f3fa0605 上有坏超级块、 缺少代码页或助手程序，或其他错误 (对某些文件系统(如 nfs、cifs) 您可能需要 一款 /sbin/mount.&lt;类型&gt; 助手程序) 分析 猜测1 可能是nfs的系统格式和集群node节点文件格式不同 1234# 查看发现nfs是ext4, 然后集群中其他的磁盘都是xfsdf -T|egrep -v "contai|var|overl"所以新挂了块磁盘,格式化为xfs然后再次实验,发现错误同样... 猜测2 可能是客户端无法识别nfs格式 12345678910111213# 做个测试mkdir /tmp/abcmount -t nfs 172.16.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-es520-2-dev-1-pvc-f462c606-5796-4c48-8928-7822f3fa0605 /tmp/abc# 果然报错mount: wrong fs type, bad option, bad superblock on 172.16.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-plugins, missing codepage or helper program, or other error (for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.&lt;type&gt; helper program) In some cases useful info is found in syslog - try dmesg | tail or so. 所以安装了nfs即可 1yum install nfs]]></content>
      <categories>
        <category>技术文档</category>
        <category>k3s</category>
        <category>nfs</category>
      </categories>
      <tags>
        <tag>k3s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s1.16使用旧yml部署配置问题]]></title>
    <url>%2F2019%2F11%2F25%2F28-k8s1-16%E4%BD%BF%E7%94%A8%E6%97%A7yml%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[使用k8s 1.16遇到的问题 1 appversion的改变12no matches for kind "StatefulSet" in version "apps/v1beta1"no matches for kind "Deployment" in version "extensions/v1beta1" 123456789101112# Deployment(extensions/v1beta1 舍弃)apiVersion: extensions/v1beta1 -&gt; apiVersion: apps/v1# StatefulSetapiVersion: apps/v1beta1 -&gt; apiVersion: apps/v1# 然后根据提示在spec 下添加selector.matchLabelsspec: replicas: 3 selector: matchLabels: app: test1]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s一条命令部署es5.2.0集群]]></title>
    <url>%2F2019%2F11%2F21%2F27-k8s%E4%B8%80%E6%9D%A1%E5%91%BD%E4%BB%A4%E9%83%A8%E7%BD%B2es5-2-0%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[由于老项目是基于es5.2.0, 所以准备在k8s基于nfs存储搭建一套,下面简单介绍 准备好环境和官方镜像12341 镜像: elasticsearch:5.2.02 k8s环境: k8s.1.103 存储: nfs storageclass 存储4 插件: ik分词压缩包(这里ik分词直接使用旧的es配置, 也可以自行下) 开始部署 部署命令1kubectl apply -f k8s-StatefulSet-es520-nfs.yml 配置文件 k8s-StatefulSet-es520-nfs.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: es520-2-dev namespace: ns-elastic5spec: serviceName: "es520-2-dev" replicas: 2 volumeClaimTemplates: - metadata: name: es520-2-dev-nfs annotations: volume.beta.kubernetes.io/storage-class: "nfs-retain" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 2Gi template: metadata: labels: app: es520-2-dev spec: containers: - name: es520-2-dev image: elasticsearch:5.2.0 ports: - containerPort: 9200 name: es520-2-9200 protocol: TCP - containerPort: 9300 name: es520-2-9300 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "500m" volumeMounts: - name: es520-2-dev-nfs mountPath: /usr/share/elasticsearch/data/ - name: es520-2-dev-cfg mountPath: /usr/share/elasticsearch/config/elasticsearch.yml subPath: elasticsearch.yml - name: es520-2-dev-jvm mountPath: /usr/share/elasticsearch/config/jvm.options subPath: jvm.options - name: es520-2-dev-plu mountPath: /usr/share/elasticsearch/plugins volumes: - name: es520-2-dev-cfg configMap: name: es520-2-dev-cfg items: - key: elasticsearch.yml path: elasticsearch.yml - name: es520-2-dev-jvm configMap: name: es520-2-dev-jvm items: - key: jvm.options path: jvm.options - name: es520-2-dev-plu nfs: server: 172.16.53.106 path: /data/nfs/k3s/ns-elastic5-es520-2-dev-nfs-plugins---kind: ServiceapiVersion: v1metadata: labels: app: es520-2-dev name: es520-2-dev namespace: ns-elastic5spec: type: NodePort ports: - name: es520-2-9200 port: 9200 targetPort: 9200 nodePort: 31201 protocol: TCP - name: es520-2-9300 port: 9300 targetPort: 9300 nodePort: 31301 protocol: TCP selector: app: es520-2-dev---apiVersion: v1kind: Servicemetadata: name: es520-2-dev-hlspec: clusterIP: None selector: app: es520-2-dev ports: - name: es520-2-9200 port: 9200 - name: es520-2-9300 port: 9300---apiVersion: v1kind: ConfigMapmetadata: name: es520-2-dev-cfg namespace: ns-elastic5data: elasticsearch.yml: | cluster.name: k8s-test-nfs network.host: 0.0.0.0 bootstrap.system_call_filter: false discovery.zen.ping.unicast.hosts: ["es520-2-dev-0.es520-2-dev:9300","es520-2-dev-1.es520-2-dev:9300","es520-2-dev-2.es520-2-dev:9300"] http.cors.enabled: true http.cors.allow-origin: "*" thread_pool.bulk.queue_size: 3000---apiVersion: v1kind: ConfigMapmetadata: name: es520-2-dev-jvm namespace: ns-elastic5data: jvm.options: | -Xms512m -Xmx512m -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+DisableExplicitGC -XX:+AlwaysPreTouch -server -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -Djdk.io.permissionsUseCanonicalPath=true -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j.skipJansi=true -XX:+HeapDumpOnOutOfMemoryError 效果图 rancher 上效果 elasticsearch-head 上效果图]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>elk</category>
        <category>elasticsearch5</category>
        <category>elk5</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>elasticsearch5</tag>
        <tag>elk</tag>
        <tag>elk5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash配置]]></title>
    <url>%2F2019%2F11%2F08%2F26-logstash%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[记录一些logstash的配置问题 想要排除某些信息官方文档 12345678if "_grokparsefailure" not in [tags] &#123; if [sqlDuring] &lt; 5 &#123; drop &#123;&#125; &#125;&#125;else &#123; drop &#123;&#125;&#125; 请务必注意 如果failure 可能会导致找不到 sqlDuring变量 而报错 1[2019-12-05T17:53:03,017][FATAL][logstash.runner ] An unexpected error occurred! &#123;:error=&gt;#&lt;NoMethodError: undefined method `&lt;' for nil:NilClass&gt;, :backtrace=&gt;["(eval):139:in `initialize'", "org/jruby/RubyArray.java:1613:in `each'", "(eval):137:in `initialize'", "org/jruby/RubyProc.java:281:in `call'", "(eval):96:in `filter_func'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:260:in `filter_batch'", "org/jruby/RubyProc.java:281:in `call'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb:186:in `each'", "org/jruby/RubyHash.java:1342:in `each'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb:185:in `each'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:258:in `filter_batch'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:246:in `worker_loop'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:225:in `start_workers'"]&#125;]]></content>
      <categories>
        <category>elk</category>
        <category>logstash</category>
      </categories>
      <tags>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux一些脚本汇总]]></title>
    <url>%2F2019%2F11%2F01%2F25-linux%E4%B8%80%E4%BA%9B%E8%84%9A%E6%9C%AC%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录一些shell脚本 1 清理es几天前的索引脚本2 从mysql导出表到clickhouse脚本 ###命令汇总 1. 生成字符串1tr -dc A-Za-z0-9_@$\%\^\/\+ &lt; /dev/urandom|head -c 16|xargs]]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些脚本汇总]]></title>
    <url>%2F2019%2F11%2F01%2F25-%E4%B8%80%E4%BA%9B%E8%84%9A%E6%9C%AC%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录一些shell脚本 1 清理es几天前的索引脚本2 从mysql导出表到clickhouse脚本]]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filebeat7收集mysql慢日志到es]]></title>
    <url>%2F2019%2F10%2F30%2F24-filebeat7%E6%94%B6%E9%9B%86mysql%E6%85%A2%E6%97%A5%E5%BF%97%E5%88%B0es%2F</url>
    <content type="text"><![CDATA[慢日志提供给开发查看, 采用elk统一提供,这里采用k8s环境搭建 原文: ELK收集mysql_slow.log其他: filebeat （7.1.0）docker容器 slowlog内容分析 5.5 版本慢查询日志 12345# Time: 191030 17:03:13# User@Host: myuser[myuser] @ [10.10.0.1]# Query_time: 3.329673 Lock_time: 0.000107 Rows_sent: 0 Rows_examined: 3971182SET timestamp=1572426193;select * from a where name = 1 limit 1; 5.6 版本慢查询日志 123456# Time: 191030 17:03:13# User@Host: myuser[myuser] @ [10.10.0.1] Id: 1111# Query_time: 3.329673 Lock_time: 0.000107 Rows_sent: 0 Rows_examined: 3971182use db_name;SET timestamp=1572426193;select * from a where name = 1 limit 1; 5.7 版本慢查询日志 12345# Time: 2019-10-06T13:25:38.703546+08:00# User@Host: myuser[myuser] @ [10.10.0.1] Id: 1111# Query_time: 3.329673 Lock_time: 0.000107 Rows_sent: 0 Rows_examined: 3971182SET timestamp=1572426193;select * from a where name = 1 limit 1; 除以上格式以外,还需要注意慢查询代码块,可能并不是每次都有 # Time 一条完整的日志：最终将以# User@Host: 开始的行，和以SQL语句结尾的行合并为一条完整的慢日志语句 开始部署filebeat7 准备镜像1docker pull store/elastic/filebeat:7.2.0 filebeat配置文件1234567891011121314151617181920filebeat.inputs:- type: log enabled: true paths: - /opt/slow.log exclude_lines: ['^\# Time'] multiline.pattern: '^\# Time|^\# User' multiline.negate: true multiline.match: after tail_files: trueoutput.elasticsearch: enabled: true hosts: ["10.0.0.100:9200"] protocol: "http" indices: - index: "es-index-name" k8s部署文件12345678910111213141516171819202122232425262728293031323334k8s-filebeat-7.2.0.ymlkind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: filebeat name: filebeat namespace: ns-elastic7spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: filebeat template: metadata: labels: elastic-app: filebeat spec: containers: - name: filebeat image: store/elastic/filebeat:7.2.0 volumeMounts: - name: filebeat-config mountPath: /usr/share/filebeat/filebeat.yml - name: mysql-dev-252 mountPath: /opt/php-mysql-dev-0-slow.log volumes: - name: filebeat-config hostPath: path: /data/k8s-container/elk-7.2.0/filebeat-7.2.0/filebeat.yml - name: mysql-dev-252 hostPath: path: /data/k8s-container/mysql5.5/slow.log]]></content>
      <categories>
        <category>k8s</category>
        <category>elk7</category>
        <category>filebeat7</category>
        <category>elk7</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>filebeat7</tag>
        <tag>elk7</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.5主与mysql5.7从部署配置]]></title>
    <url>%2F2019%2F10%2F29%2F23-mysql5-5%E4%B8%BB%E4%B8%8Emysql5-7%E4%BB%8E%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[由于需要将旧版mysql5.5的数据同步到新mysql5.7, 并且会对部分表分库 参考教程: mysql从5.5直接升级到5.7 mysql5.5升级到mysql5.7 采用mysql5.5数据目录升级为mysql5.712345678910111 从mysql5.5的从库 copy /data数据2 修改新的mysql5.7配置文件 my.cnf，添加datadir，指向5.5数据目录3 新安装数据库执行(本次不需要执行) /usr/local/mysql57/bin/mysqld --defaults-file=/etc/my57.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/disk/u014 启动mysql5 此时数据目录还是5.5的，需要执行mysql_upgrade进行升级，在执行表修复前，需要确认一个参数innodb_file_per_table，mysql官网对该参数的解释如下 该参数在5.5版本默认为OFF，所有表和索引都导入一个共享文件中，名为ibdata1,但在5.6.7及以后版本，改参数被默认设置为ON，即每张表都有对应的表和索引存储文件，每个schema下，每个frm文件都有对应的ibd文件。 在执行mysql_upgrade时，会修复系统表，并且如果该参数在5.5和5.7版本均使用默认值，则会将之前共享表和索引的存储方式改为每张表单独存储表和索引的形式，故会出现拷贝复制的操作，如果数据量比较大，则用时就会很长， 使用nnodb_file_per_table=1，及表和索引单独存储的优缺点，可查看mysql官网介绍。6 使用mysql_upgrade检测并修复表 /usr/local/mysql57/bin/mysql_upgrade -S /disk/u01/mysql.sock 以上已经完成对mysql5.5数据升级 在mysql5.7运行的功能 配置mysql5.5主与mysql5.7从 将msyql5.7作为mysql5.5的从库123456# 从库执行, POS位置以 show master status\G 查询为准stop slave;SET GLOBAL read_only=0;reset slave all;CHANGE MASTER TO MASTER_HOST='db_master.prod.boqii.com',MASTER_PORT=3306,MASTER_USER='boqii_repl_user',MASTER_PASSWORD='d79f25dfb70f315819edaa1d',MASTER_LOG_FILE='m1-master-bin.000001',MASTER_LOG_POS=107;start slave; 在主库测试创建表, 查看是否会同步到mysql5.7从库1234567create table tutorials_tbl( tutorial_id INT NOT NULL AUTO_INCREMENT, tutorial_title VARCHAR(100) NOT NULL, tutorial_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY ( tutorial_id )); 修改mysql5.7库名 没问题之后,我们需要将mysql5.7的mydatabase库改成mydatabasenew库名, 断开mysql5.5 和mysql5.7主从同步(最好设置mysql5.5只读,防止数据差异), 在mysql5.7上执行改库名, 以下有触发器的表会修改失败 测试执行时间在15s左右 123456789101112#!/bin/bash# 假设将sakila数据库名改为new_sakila# MyISAM直接更改数据库目录下的文件即可new_database=mydatabasenewold_database=mydatabasemysql -S /disk/u01/mysql.sock -e 'create database if not exists $&#123;new_database&#125;'list_table=$(mysql -S /disk/u01/mysql.sock -Nse "select table_name from information_schema.TABLES where TABLE_SCHEMA='$&#123;old_database&#125;'")for table in $list_tabledo mysql -S /disk/u01/mysql.sock -e "rename table $&#123;old_database&#125;.$table to $&#123;new_database&#125;.$table"done 此时在配置新的mysql5.7的主从机器1234567891011121314```&lt;center&gt;&lt;img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/&gt;&lt;font color="blue" face="黑体" size=5&gt; 一些配置问题 &lt;/font&gt;&lt;/center&gt;---### GTID_MODE 配置不统一 The replication receiver thread cannot start because the master has GTID_MODE = OFF and this server has GTID_MODE = ON. 永久修改gtid_mode = off 一次性关闭步骤：stop slave;SET GLOBAL GTID_MODE = ‘ON_PERMISSIVE’;SET GLOBAL GTID_MODE = ‘OFF_PERMISSIVE’;SET GLOBAL GTID_MODE = ‘OFF’;start slave; 12### mysql5.7 sql_mode sql_mode=’ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION’ 1234---### 注意一台机器多个mysql启动脚本修改问题 #以下两处修改 /etc/init.d/mysqld57parse_server_arguments $print_defaults -c /etc/my57.cnf mysqld server mysql_server mysql.server$bindir/mysqld_safe –defaults-file=/etc/my57.cnf –pid-file=”$mysqld_pid_file_path” $other_args &gt;/dev/null &amp; 12345678&lt;center&gt;&lt;img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/&gt;&lt;font color="blue" face="黑体" size=5&gt; 一些info信息 &lt;/font&gt;&lt;/center&gt;### /usr/local/mysql57/bin/mysql_upgrade -S /disk/u01/mysql.sock 的部分记录 /usr/local/mysql57/bin/mysql_upgrade -S /disk/u01/mysql.sockChecking if update is needed.Checking server version.Running queries to upgrade MySQL server.mysql_upgrade: (non fatal) [WARNING] 1642: Pre-4.1 password hash found. It is deprecated and will be removed in a future release. Please upgrade it to a new format.Checking system database.mysql.columns_priv OKmysql.db OKmysql.engine_cost OKmysql.event OKmysql.func OKmysql.general_log OKmysql.gtid_executed OKmysql.help_category OKmysql.help_keyword OKmysql.help_relation OKmysql.help_topic OKmysql.host OKmysql.innodb_index_stats OKmysql.innodb_table_stats OKmysql.ndb_binlog_index OKmysql.plugin OKmysql.proc OKmysql.procs_priv OKmysql.proxies_priv OKmysql.server_cost OKmysql.servers OKmysql.slave_master_info OKmysql.slave_relay_log_info OKmysql.slave_worker_info OKmysql.slow_log OK… 12### 附录 my57.cnf [client]port = 3308socket = /disk/u01/mysql.sock [mysql]prompt=”\u@m1_618_u [\d]&gt; “no-auto-rehash [mysqld]sql_mode=’ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION’replicate-wild-do-table=mydatabase.% binlog-ignore-db=information_schemabinlog-ignore-db=mysqlbinlog-ignore-db=performance_schemabinlog-ignore-db=test binlog-do-db=mydatabase user = mysqlport = 3308basedir = /usr/local/mysql57datadir = /disk/u01socket = /disk/u01/mysql.sockpid-file = /disk/u01/dbm1_u.pidtmpdir = /disk/u02server-id = 123character-set-server = utf8skip_name_resolve = 1innodb_file_per_table = 1explicit_defaults_for_timestamp = 0read_only = 1 buffer&amp;cachetable_open_cache = 100table_definition_cache = 400table_open_cache_instances = 64sort_buffer_size = 4Mjoin_buffer_size = 4Mread_buffer_size = 8Mread_rnd_buffer_size = 4M thread&amp;connectionthread_stack = 256Kthread_cache_size = 768back_log = 1024max_connections = 3000max_connect_errors = 1000000 temptabletmp_table_size = 32Mmax_heap_table_size = 32M networkmax_allowed_packet = 32M #lock_wait_timeout = 3600 #interactive_timeout = 600 #wait_timeout = 600 query cachequery_cache_size = 0query_cache_type = 0 设置errorlog、slowlog和generallog的时区，默认UTClog_timestamps = SYSTEM error-loglog_error = /disk/u02/mysqld.log slow-logslow_query_log = 1slow_query_log_file = /disk/u02/slow.loglong_query_time = 0.1log_queries_not_using_indexes =1log_throttle_queries_not_using_indexes = 60min_examined_row_limit = 100log_slow_admin_statements = 1log_slow_slave_statements = 1 general log#general-log = 1general_log_file=/disk/u02/query.log binlogbinlog_format = rowbinlog_checksum = 1log-bin = /disk/u02/m1-binlog-bin-index = /disk/u02/m1-bin.indexsync_binlog = 0binlog_cache_size = 4Mmax_binlog_cache_size = 1Gmax_binlog_size = 512Mexpire_logs_days = 15 GTIDgtid_mode = offenforce_gtid_consistency = 1log_slave_updates Replicationmaster_info_repository = TABLErelay_log_info_repository = TABLEslave-rows-search-algorithms = ‘INDEX_SCAN,HASH_SCAN’relay_log_recovery = 1relay_log_purge = 1relay-log=/disk/u02/m1-relay-binrelay-log-index=/disk/u02/m1-relay-bin.index innodb-buffer&amp;cacheinnodb_buffer_pool_size = 1Ginnodb_buffer_pool_instances = 4 #innodb_additional_mem_pool_size = 16Minnodb_max_dirty_pages_pct = 50 innodb loginnodb_data_file_path = ibdata1:256M:autoextendinnodb_log_file_size = 256Minnodb_log_files_in_group = 2innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 32M #innodb_max_undo_log_size = 4G #innodb_undo_directory = undologinnodb_undo_tablespaces = 0 innodb-ioinnodb_flush_method = O_DIRECTinnodb_io_capacity = 600innodb_io_capacity_max = 2000innodb_flush_sync = 0innodb_flush_neighbors = 0 #innodb_lru_scan_depth = 4000innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_purge_threads = 4innodb_page_cleaners = 4 transaction,lock#innodb_sync_spin_loops = 100 #innodb_spin_wait_delay = 30innodb_lock_wait_timeout = 10innodb_print_all_deadlocks = 1innodb_rollback_on_timeout = 1 innodb_open_files = 65535 innodb_online_alter_log_max_size = 1G innodb statusinnodb_status_file = 1 注意: 开启 innodb_status_output &amp; innodb_status_output_locks 后, 可能会导致log-error文件增长较快innodb_status_output = 0innodb_status_output_locks = 0 #performance_schemaperformance_schema = 1performance_schema_instrument = ‘%=on’ #innodb monitorinnodb_monitor_enable=”module_innodb”innodb_monitor_enable=”module_server”innodb_monitor_enable=”module_dml”innodb_monitor_enable=”module_ddl”innodb_monitor_enable=”module_trx”innodb_monitor_enable=”module_os”innodb_monitor_enable=”module_purge”innodb_monitor_enable=”module_log”innodb_monitor_enable=”module_lock”innodb_monitor_enable=”module_buffer”innodb_monitor_enable=”module_index”innodb_monitor_enable=”module_ibuf_system”innodb_monitor_enable=”module_buffer_page”innodb_monitor_enable=”module_adaptive_hash” MyISAMkey_buffer_size = 1024Mbulk_insert_buffer_size = 64Mmyisam_sort_buffer_size = 128Mmyisam_repair_threads = 1 [mysqldump]quickmax_allowed_packet = 32M]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
        <category>主从</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es5集群磁盘扩容]]></title>
    <url>%2F2019%2F10%2F28%2F22-es%E9%9B%86%E7%BE%A4%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[es集群磁盘不足,对磁盘扩容遇到一些的问题 重启集群前，先设置集群停止分片移动：12345curl -XPUT http://localhost:9200/_cluster/settings -d '&#123;"transient" : &#123;"cluster.routing.allocation.enable" : "none"&#125;&#125;' 对磁盘进行扩容,每次操作一个节点123456789101112131415# 直接扩容磁盘到2T//针对ext4文件格式的操作系统（如CentOS6）：//umount /dev/vdbe2fsck -f /dev/vdbresize2fs /dev/vdbmount /dev/vdb /data# 或者新增 2T云盘/dev/vdcumount /data/mkdir /data2mount /dev/vdb /data2mkfs.ext4 /dev/vdcmount /dev/vdc /datacp -ra /data2/* /data/ 重启之后，恢复分片自动分配：12345curl -XPUT http://localhost:9200/_cluster/settings -d '&#123;"transient" : &#123;"cluster.routing.allocation.enable" : "all"&#125;&#125;' 如果需要下线其中的节点, 先将分片都转义到其他节点123456# 执行以下命令会自动将10.10.0.1 节点上的分片全部迁移到其他机器, 等待迁移完成, 将改空机器下线即可curl -XPUT 127.0.0.1:9200/_cluster/settings -d '&#123;"transient" :&#123;"cluster.routing.allocation.exclude._ip" : "10.10.0.1"&#125;&#125;' 另外对于 path.data 配置多快盘的问题1234比如es8配置了三块盘:/disk4/data -&gt; sde, /disk5/data -&gt; sdf, disk6/data -&gt; sdg这里注意 es node的data path尽量保证盘的大小差别不要太大, sde,sdf,sdg的大小保障差不多, 否则由于es shard 均衡的时候可能会优先分配到磁盘大的目录, 可能会导致sde(假如这个磁盘最大)的IO高, 而sdf等IO低 简单的配置信息elasticsearch512345678910111213cluster.name: es-devnode.name: es1-upath.data: /data/es/datapath.logs: /data/es/logsnetwork.host: 0.0.0.0discovery.zen.ping.unicast.hosts: ["10.10.0.1:9300","10.10.0.2:9300","10.10.0.3:9300","10.10.0.4:9300"]http.cors.enabled: truehttp.cors.allow-origin: "*"xpack.security.enabled: falsebootstrap.system_call_filter: falsethread_pool.bulk.queue_size: 3000# 防止脑裂discovery.zen.minimum_master_nodes: 2]]></content>
      <categories>
        <category>elk</category>
        <category>elasticsearch5</category>
      </categories>
      <tags>
        <tag>elasticsearch5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流量复制工具gor]]></title>
    <url>%2F2019%2F10%2F28%2F21-%E6%B5%81%E9%87%8F%E5%A4%8D%E5%88%B6%E5%B7%A5%E5%85%B7gor%2F</url>
    <content type="text"><![CDATA[Gor 是一款go语言实现的简单的http流量复制工具，它的主要目的是使你的生产环境HTTP真实流量在测试环境和预发布环境重现 流量复制工具 下载安装github下载地址: https://github.com/buger/goreplay/releases 1234tar -xvf gor_1.0.0_x64.tar.gzmv gor /usr/bin/which gor 命令123456789101112131415161718192021222324252627282930311 保存请求到文件# 将本机所有80请求保存到gor-20171120_0.log文件(注意会生成很多文件)gor --input-raw :80 --output-file gor-%Y%m%d.log# --output-file-append 会生成gor-20171120.log文件gor --input-raw :80 --output-file gor-%Y%m%d.log --output-file-append2 根据文件回放请求# 镜像qps回放gor --input-file gor-aaaa-20171120.log --output-http aaaa-dev.test.com# 两倍镜像qps回放gor --input-file "gor-aaaa-20171120.log|200%" --output-http aaaa-dev.test.com3 过滤url后保存请求到文件# 排除s.test.com的请求gor --input-raw :80 --output-file gor-%Y%m%d.log --output-file-append --http-disallow-header "Host: s.test.com" --http-disallow-header "Host: www.test.com" --http-disallow-header "Host: bbs.test.com"# 只存储aaaa.test.com的请求gor --input-raw :80 --output-file gor-aaaa-%Y%m%d.log --output-file-append --http-allow-header "Host: aaaa.test.com"# https的不能抓包gor --input-raw :443 --output-file gor-ssl-aaaa-%Y%m%d.log --output-file-append --http-allow-header "Host: aaaa.test.com"4 在线镜像复制请求# 将生产aaaa.test.com的请求复制到 aaaa-dev.test.com 环境!gor --input-raw :80 --output-http "aaaa-dev.test.com" --http-allow-header "Host: aaaa.test.com" 离线文件编辑123456文件的每个请求通过 如下字符串分割!ð&lt;9f&gt;&lt;90&gt;µð&lt;9f&gt;&lt;99&gt;&lt;88&gt;ð&lt;9f&gt;&lt;99&gt;&lt;89&gt;并且第一行是 请求的唯一码? 和时间戳!1 9b366a8eab8d6cb8e557cb3bf43f69c36612cffb 1511165572419843000所以可录制比如半小时的然后窃取需要的时间段! 问题 https 不能抓包! 通过添加代理, gor抓取8000端口 1234567891011121314151617181920212223# SSL terminationserver &#123; listen 443 ssl; server_name aaaa.test.com; ssl_certificate /etc/ssl/nginx/server.crt; ssl_certificate_key /etc/ssl/nginx/server.key; location / &#123; proxy_set_header Host $host; proxy_pass http://localhost:8000; &#125;&#125;server &#123; listen 8000; server_name aaaa.test.com; location / &#123; proxy_set_header Host $host; proxy_pass http://production_shop_api_site; &#125;&#125;]]></content>
      <categories>
        <category>流量复制工具</category>
        <category>gor</category>
      </categories>
      <tags>
        <tag>gor</tag>
        <tag>http流量复制工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s搭建mysql5.7.24主从]]></title>
    <url>%2F2019%2F10%2F24%2F20-k8s%E6%90%AD%E5%BB%BAmysql5-7-24%E4%B8%BB%E4%BB%8E%2F</url>
    <content type="text"><![CDATA[k8s上简单部署mysql5.7.24主从 k8s搭建mysql5.7.24主从 参考文档利用Kubernetes搭建mysql主从复制集群官方dockerfile 从hub.docker.com拉取官方镜像1docker pull mysql:5.7.24 build镜像 主库master的Dockerfile12345from mysql:5.7.24run sed -i '/\[mysqld\]/a server-id=1\nlog-bin' /etc/mysql/mysql.conf.d/mysqld.cnfCOPY docker-entrypoint.sh /usr/local/bin/ 主库的docker-entrypoint.sh 先从初始镜像取 或者从github对应版本上 123docker run -dti mysql:5.7.24 /bin/bashdocker cp 2bfa6209d120c23:/usr/local/bin/docker-entrypoint.sh . 修改docker-entrypoint.sh 12345678fi# 添加以下内容echo "CREATE USER '$MYSQL_REPLICATION_USER'@'%' IDENTIFIED BY '$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;"echo "GRANT REPLICATION SLAVE ON *.* TO '$MYSQL_REPLICATION_USER'@'%' IDENTIFIED BY '$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;"echo "FLUSH PRIVILEGES ;" | "$&#123;mysql[@]&#125;"# 添加以上内容echo ls /docker-entrypoint-initdb.d/ &gt; /dev/null build主库镜像 12docker build -t hub.boqii.com/bq/mysql-master:5.7.24 .docker push hub.boqii.com/bq/mysql-master:5.7.24 从库的docker-entrypoint.sh 同上先从初始镜像取 或者从github对应版本上 或复制上面的文件 修改docker-entrypoint.sh 12345678fi# 添加以下内容 echo "STOP SLAVE;" | "$&#123;mysql[@]&#125;" echo "CHANGE MASTER TO master_host='$MYSQL_MASTER_SERVICE_HOST', master_user='$MYSQL_REPLICATION_USER', master_password='$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;" echo "START SLAVE;" | "$&#123;mysql[@]&#125;" # 添加以上内容echo ls /docker-entrypoint-initdb.d/ &gt; /dev/null build从库镜像 12docker build -t hub.boqii.com/bq/mysql-slave:5.7.24 .docker push hub.boqii.com/bq/mysql-slave:5.7.24 开始部署 k8s-master-mysql_5.7.24.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869---apiVersion: apps/v1beta1kind: StatefulSetmetadata: labels: app: php-mysql-master-dev name: php-mysql-master-dev namespace: dbspec: serviceName: "php-mysql-master-dev" replicas: 1 selector: matchLabels: app: php-mysql-master-dev template: metadata: labels: app: php-mysql-master-dev spec: containers: - name: php-mysql-master-dev image: hub.boqii.com/bq/mysql-master:5.7.24 ports: - containerPort: 3306 name: db-port resources: requests: cpu: "50m" limits: cpu: "1000m" env: - name: MYSQL_ROOT_PASSWORD value: "Boqii.123" - name: MYSQL_REPLICATION_USER value: "repl" - name: MYSQL_REPLICATION_PASSWORD value: "7a5b21ac65712bd95e39d3c1" volumeMounts: - name: order-master-dev-data mountPath: /var/lib/mysql - name: order-master-dev-cfg mountPath: /etc/mysql volumes: - name: order-master-dev-data hostPath: path: /data/k8s-container/php-mysql-dev/master/data - name: order-master-dev-cfg hostPath: path: /data/k8s-container/php-mysql-dev/master/etc-mysql---kind: ServiceapiVersion: v1metadata: labels: app: php-mysql-master-dev name: php-mysql-master-dev-service namespace: dbspec: type: NodePort ports: - port: 3306 name: db-port targetPort: 3306 nodePort: 23306 protocol: TCP selector: app: php-mysql-master-dev k8s-slave-mysql_5.7.24.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172---apiVersion: apps/v1beta1kind: StatefulSetmetadata: labels: app: php-mysql-slave-dev name: php-mysql-slave-dev namespace: dbspec: serviceName: "php-mysql-slave-dev" replicas: 1 selector: matchLabels: app: php-mysql-slave-dev template: metadata: labels: app: php-mysql-slave-dev spec: containers: - name: php-mysql-slave-dev image: hub.boqii.com/bq/mysql-slave:5.7.24 ports: - containerPort: 3306 name: db-port resources: requests: cpu: "50m" limits: cpu: "1000m" env: - name: MYSQL_ROOT_PASSWORD value: "Boqii.123" - name: MYSQL_REPLICATION_USER value: "repl" - name: MYSQL_REPLICATION_PASSWORD value: "7a5b21ac65712bd95e39d3c1" - name: MYSQL_MASTER_SERVICE_HOST value: "php-mysql-master-dev-service" volumeMounts: - name: order-slave-dev-data mountPath: /var/lib/mysql - name: order-slave-dev-cfg mountPath: /etc/mysql volumes: - name: order-slave-dev-data hostPath: path: /data/k8s-container/php-mysql-dev/slave/data - name: order-slave-dev-cfg hostPath: path: /data/k8s-container/php-mysql-dev/slave/etc-mysql---kind: ServiceapiVersion: v1metadata: labels: app: php-mysql-slave-dev name: php-mysql-slave-dev-service namespace: dbspec: type: NodePort ports: - port: 3306 name: db-port targetPort: 3306 nodePort: 23307 protocol: TCP selector: app: php-mysql-slave-dev 问题总结 从库的replay log名字会根据docker主机名变化, 也可以写在配置文件 12# Dockerfile中可以添加run sed -i '/\[mysqld\]/a relay-log-index=php-mysql-shoporder-slave-dev-relay-bin.index' /etc/mysql/mysql.conf.d/mysqld.cnf 注意MYSQL_MASTER_SERVICE_HOST 变量的配置, 根据你master的service变化 其次我docker-entrypoint.sh 文件几次手动从页面复制粘贴下来的导致各种语法错误,这里建议找到对的版本从github克隆, 或者从mysql:5.7.24镜像中cp 配置etc-mysql/mysql.conf.d/mysqld.cnf 123456789[mysqld]# 从库配置read_only=1super_read_only=1character-set-server=utf8# 1 去掉STRICT_TRANS_TABLES 表NOT NULL时无法创建表# 2 修改NO_ZERO_DATE为ALLOW_INVALID_DATES 允许’0000-00-00’#sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'sql_mode='ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' 配置etc-mysql/conf.d/mysql.cnf 123[mysql]no-auto-rehashdefault-character-set=utf8 附录master配置docker-entrypoint.shslave配置docker-entrypoint.sh]]></content>
      <categories>
        <category>mysql</category>
        <category>k8s</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell中gt和>的区别]]></title>
    <url>%2F2019%2F10%2F17%2F19-shell%E4%B8%ADgt%E5%92%8C%3E%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[shell中 gt 和 &gt; 的一些相关问题介绍和测试 以下是bash的测试, 注意如果你是zsh可能会不同喔😯 [[]] , [] 和test比较 [] 和test: 两者是一样的，在命令行里test expr和[ expr ]的效果相同。test中可用的比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq, -gt这种形式。通过which [ 和which test 可以看到是命令 [] 和test 例子 1234567[root@dk-centos6 ~]# a="abcdef"[root@dk-centos6 ~]# test "$a" = "abcdef"[root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ "$a" = "abcdef" ][root@dk-centos6 ~]# echo $?0 [[ ]]具体功能: [[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 支持字符串的模式匹配（使用=~操作符时甚至支持shell的正则表达 式）,右边的字符串不加双引号的情况,可以把右边作为模式. 比如[[ hello == hell? ]]，结果为真。当然加引号就是文本字符串比较. 使用[[ … ]]条件判断结构，而不是[ … ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用if [[ $a != 1 &amp;&amp; $a != 2 ]], 如果不适用双括号, 则为if [ $a -ne 1] &amp;&amp; [ $a != 2 ]或者if [ $a -ne 1 -a $a != 2 ]。 纯数字比较 &gt; 通过比较ASCII值,gt仅能比较数字123456789101112[root@dk-centos6 ~]# [ 2 \&gt; 1 ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ 2 -gt 1 ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [[ 2 &gt; 1 ]][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [[ 2 -gt 1 ]][root@dk-centos6 ~]# echo $?0 字符串比较 单括号中如果要比较符号 “&lt;” “&gt;”, 需要转义, 否则判断结果错误123456789[root@dk-centos6 ~]# [ "b" &gt; "a" ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ "b" &lt; "a" ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ "b" \&lt; "a" ][root@dk-centos6 ~]# echo $?1 双括号不用转义 , 直接执行即可123456[root@dk-centos6 ~]# [[ "b" &gt; "a" ]][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [[ "b" &lt; "a" ]][root@dk-centos6 ~]# echo $?1]]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[收藏链接]]></title>
    <url>%2F2019%2F10%2F17%2F18-%E6%94%B6%E8%97%8F%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[记录一些好的网址 容器 云原生图书编年史 云原生大佬博客汇总 (2000个) kubernetes 社群分享 QA 汇总 Linux服务 MySQL 5.6, 5.7, 8.0版本的新特性大全 nginx+lua实现灰度发布 golang go学习指南]]></content>
      <categories>
        <category>网址</category>
        <category>收藏</category>
      </categories>
      <tags>
        <tag>收藏</tag>
        <tag>网址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown一些写法记录]]></title>
    <url>%2F2019%2F10%2F16%2F17-markdown%E4%B8%80%E4%BA%9B%E5%86%99%E6%B3%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[记录一些需要注意的写法,markdown 是支持html语法的 这里想写一个居中的标题和图片1234&lt;center&gt;&lt;img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/&gt;&lt;font color="blue" face="黑体" size=5&gt; Dockerfile &lt;/font&gt;&lt;/center&gt; 以下即效果图: 这个就是效果图咯]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile介绍]]></title>
    <url>%2F2019%2F10%2F16%2F16-dockerfile%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Dockerfile 本文摘录于: 如何快速将容器云镜像大小精简98%？ Dockerfile 文件有自己的书写格式和支持的命令，常用的Dockerfile 指令有： FROM 指定基镜像。 MAINTAINER 设置镜像的作者信息，如作者姓名、邮箱等。 COPY 将文件从本地复制到镜像，拷贝前需要保证本地源文件存在。 ADD 与 COPY 类似，复制文件到镜像。不同的是，如果文件是归档文件（tar, zip, tgz, xz 等），会被自动解压。 ENV 设置环境变量，格式: ENV key=value或ENV key value，运行容器后，可直接在容器中使用。 EXPOSE 暴露容器中指定的端口，只是一个声明，主要用户了解应用监听的端口。 VOLUME 挂载卷到容器，需要注意的是，保存镜像时不会保存卷中的数据。 WORKDIR 设置当前工作目录，后续各层的当前目录都被指定。 RUN 在容器中运行指定的命令。 CMD 容器启动时运行的命令。Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效。CMD 可以被 docker run 之后的参数替换。 ENTRYPOINT 设置容器启动时运行的命令。Dockerfile 中可以有多个 ENTRYPOINT 指令，但只有最后一个生效。CMD 或 docker run 之后的参数会被当做参数传递给 ENTRYPOINT，这个是与CMD的区别。 容器的原理 容器镜像中最重要的概念就是layers，即镜像层。 容器的原理 镜像层依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载(union mounts)等技术查看Docker 官方文档https://docs.docker.com/storage/storagedriver/进行学习。 每条指令都创建一个镜像层，会增加镜像的大小 下面看个例子这里我有一个1.2M的镜像 12docker images|grep busyboxbusybox latest 19485c79a9bb 5 weeks ago 1.22MB 我们基于busybox写一个Dockerfile来build 1234567#cat Dockerfilefrom busybox:latestrun mkdir /tmp/dir \ &amp;&amp; dd if=/dev/zero of=/tmp/dir/file1 bs=1M count=10run rm -f /tmp/dir/file1 执行build 1234567891011121314151617docker build -t busybox-test .Sending build context to Docker daemon 2.048kBStep 1/3 : from busybox:latest ---&gt; 19485c79a9bbStep 2/3 : run mkdir /tmp/dir &amp;&amp; dd if=/dev/zero of=/tmp/dir/file1 bs=1M count=10 ---&gt; Running in 0426f92c77ed10+0 records in10+0 records out10485760 bytes (10.0MB) copied, 0.003785 seconds, 2.6GB/sRemoving intermediate container 0426f92c77ed ---&gt; 5ec75db090c9Step 3/3 : run rm -f /tmp/dir/file1 ---&gt; Running in 540e7d0a5aeaRemoving intermediate container 540e7d0a5aea ---&gt; 00041489cc0eSuccessfully built 00041489cc0eSuccessfully tagged busybox-test:latest 查看image大小 123docker images|grep busyboxbusybox-test latest 00041489cc0e 10 minutes ago 11.7MBbusybox latest 19485c79a9bb 5 weeks ago 1.22MB ??? 我不是rm删除了创建的/tmp/dir/file1 文件吗? 难道它还在? 来,我们测试一下 12# 查看目录下是否有文件docker run -ti busybox-test ls /tmp/dir 结果显然是空… 喔,,, 因为”在Dockerfile中，每条指令都会创建一个镜像层，继而会增加镜像整体的大小”, 在看我们写的Dockerfile,我们第一个run 执行的时候, 这里假装叫 (run1层), 我们生成了file1文件当执行第二个run的时候, 我们处在了 (run2层), (run1层)已经是父层,是个只读层了,只有当前层可写, 虽然我们在 (run2层)删除了这个文件,但删除的仅仅是份拷贝而已, 这就是写时复制. 所以以上的优化应该是: 写成一条run 123456789#cat Dockerfilefrom busybox:latestrun mkdir /tmp/dir \ &amp;&amp; dd if=/dev/zero of=/tmp/dir/file1 bs=1M count=10 \ &amp;&amp; rm -f /tmp/dir/file1# builddocker build -t busybox-test2 . 结果显然 1234docker images|grep busyboxbusybox-test2 latest faf8b7d4f140 3 seconds ago 1.22MBbusybox-test latest 00041489cc0e 10 minutes ago 11.7MBbusybox latest 19485c79a9bb 5 weeks ago 1.22MB 虽然说这里的测试没有干任何事情, 但我们在写Dockerfile的时候需要注意, 两个run之间是两个不同的 可写层! 简单总结精简镜像大小的方法: 121 使用更小的基础镜像,注意一些很小的镜像可能缺少很多依赖库,例如查看redis依赖库 ldd /usr/bin/redis-cli2 合并Dockerfilec指令精简(可以的话写成一条run) 一些小的镜像 1 scratch: 一个空的镜像, 无法pull -.-!!! , 写在Dockerfile是可以的 2 alpine: 5M的linux镜像,有包管理工具apk 123FROM scratchADD alpine-minirootfs-3.10.2-x86_64.tar.gz /CMD ["/bin/sh"] 3 busybox: 1M多的镜像,称为嵌入式linux的瑞士军刀, Linux和unix一些常用的命令]]></content>
      <categories>
        <category>docker</category>
        <category>Dockerfile</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker简介和使用]]></title>
    <url>%2F2019%2F10%2F16%2F15-docker%E7%AE%80%E4%BB%8B%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简单介绍docker dockerk8s支持的docker 版本查看这里我们使用年份命名版本的docker-ce，假设我们要安装v1.16.3的k8s，我们去 https://github.com/kubernetes/kubernetes 里进对应版本的CHANGELOG-1.16.md里搜The list of validated docker versions remain查找支持的docker版本，docker版本不一定得在支持列表里，实际上19.03也能使用，这里我们使用docker官方的安装脚本安装docker(该脚本支持centos和ubuntu) 12export VERSION=19.03curl -fsSL "https://get.docker.com/" | bash -s -- --mirror Aliyun]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.5目录copy方式迁移]]></title>
    <url>%2F2019%2F10%2F15%2F14-mysql%E7%9B%AE%E5%BD%95copy%E6%96%B9%E5%BC%8F%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[从现有的一台 从库 全copy data目录到2台新机器上, 再配置mysql主从 目录copy方式迁移 注意 不要删除ibdata1,会导致innodb表不存在 可以不删除ib_logfile0,ib_logfile1, 但my.cnf配置大小一致 区别目录权限为mysql,tmp目录存在 请自行安装好mysql5.5 1234567891 首先停止mysql/etc/init.d/mysqld stop2 同步数据目录到新机器/data/u013 确认新机器上mysql版本并配置/etc/my.cof4 完整迁移时不需要删除内容(innodb_log_file_size = 256M 配置要一致)5 启动mysql 配置主从 1 首先 目录copy方式 同步某个从库到2台新机器并启动完成, 此时两个mysql都开启了slave 2 暂停同步，并设置读写； 123456stop slave;# 该执行仅主库上执行(配置可写)SET GLOBAL read_only=0;reset slave all;-- RESET SLAVE ALL是清除从库的同步复制信息、包括连接信息和二进制文件名、位置-- 从库上执行这个命令后，使用show slave status将不会有输出。 3 2台新的mysql中修改从库slave配置, 连接到新的主库地址(我这里通过域名解析) 12CHANGE MASTER TO MASTER_HOST='a_master.b.com',MASTER_PORT=3306,MASTER_USER='repl_user',MASTER_PASSWORD='xxxx',MASTER_LOG_FILE='m1-master-bin.000001',MASTER_LOG_POS=88; 4 由于本机需要安装mysql5.5和mysql5.7所以注意一下 123456# 初始化指定配置文件/usr/local/mysql57/bin/mysqld --defaults-file=/etc/my57.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql57 --datadir=/data/u001# 修改/etc/init.d/mysqld57parse_server_arguments `$print_defaults -c /etc/my57.cnf mysqld server mysql_server mysql.server`$bindir/mysqld_safe --defaults-file=/etc/my57.cnf --pid-file="$mysqld_pid_file_path" $other_args &gt;/dev/null &amp; 报错统计 ERROR 1840 (HY000) at line 24: @@GLOBAL.GTID_PURGED can only be set when @@GLOBAL.GTID_EXECUTED is empty. 1执行reset master; The MySQL server is running with the–read-only option so it cannot execute this statement 1执行 set global read_only=0;]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云原生博客汇总]]></title>
    <url>%2F2019%2F10%2F12%2F13-%E4%BA%91%E5%8E%9F%E7%94%9F%E5%8D%9A%E5%AE%A2%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录一些云原生技术博客 来自ServiceMesher[8] 群KaiRen’s Blog Zlatan Eevee 国南之境 博客 | 高策 Kakashi’s Blog Alex Wu’s blog | THINK BIG, START SMALL, DELIVER VALUE TO THE BUSINESS 开元DevOps知识库 - 知识管理，时间管理，自我管理 起风了 茶歇驿站 - Gopher, OpenSource Fans, 成长之路有我相伴。 Polar Snow Documentation 云原生实验室 - 米开朗基杨的博客 Jimmy Song - 宋净超的博客|Cloud Native|云原生布道师 漠然的博客 | mritd Blog DevOps – 成长之路 birdben 浮生若梦 CNCF Cloud Native Interactive Landscape 杜屹东的博客 | 学无止境 梦旭随想 ictfox blog CloudNative 架构 我爱西红柿 Doublemine Bingo Huang Arthur Chunqi Li’s Blog Archive | Arthur Chunqi Li’s Blog IT技术工作学习折腾笔记 李佶澳的博客 墨荷琼林官网-编程日志 Archive - Nolla Tomoya’s Blog 君无止境 Jamin Zhang roc - imroc.io|roc的博客|Cloud Native|Kubernetes|Go|Golang Blog | Sysdig sleele的博客 TauCeti blog · TauCeti blog 水晶命匣 | 生命在于折腾，折腾万岁！ 苏洋博客 LinuxTOY Infvie’s Blog | 运维SRE社区博客 Solar MoeLove Hwchiu Learning Notekubernetes/SDN/DevOps 存储领域 yangguanjun]]></content>
      <categories>
        <category>技术文档</category>
        <category>网址</category>
        <category>大佬博客</category>
      </categories>
      <tags>
        <tag>云原生</tag>
        <tag>cloud native</tag>
        <tag>大佬博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk简单记录]]></title>
    <url>%2F2019%2F10%2F11%2F12-awk%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[记录一些简单使用 实例1: 计算nginx日志中某个接口的次数和平均响应时间例如我的a.txt nginx日志格式如下12a.b.com 1.1.1.1 [08/Sep/2019:23:57:01 +0800] "GET /v1/actionname?xxxx HTTP/1.1" 200 386 "-" "Mozilla/5.0 (Linux; Android 9; V1831A Build/P00610; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/68.0.3440.91 Mobile Safari/537.36" "-" "0.023"a.b.com 1.1.1.1 [08/Sep/2019:23:57:01 +0800] "GET /v1/actionname2?xxxx HTTP/1.1" 200 386 "-" "Mozilla/5.0 (Linux; Android 9; V1831A Build/P00610; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/68.0.3440.91 Mobile Safari/537.36" "-" "0.016" 这里我只想取出接口名: /v1/actionname 和 0.023 响应时间 首先我取出这两列1234567cat a.txt|awk -F '"' '&#123;print $(NF-1),$2&#125;'|awk -F '?' '&#123;print $1&#125;'|awk '&#123;print $1" "$3&#125;' &gt; b.txtcat b.txt0.023 /v1/actionname0.016 /v1/actionname2... 命令详解12345678&gt; 第一步 响应时间求和&#123;s[$2]+=$1&#125;: 每遇到一个$2,比如遇到/v1/actionname,记录一个数组s[/v1/actionname] = 所有$1的值的总和&gt; 第二步 算接口的次数&#123;m[$2]++&#125;: 每遇到一个$2,比如遇到/v1/actionname,记录一个数组m[/v1/actionname] = 所有$1的个数&gt; 第三步 取平均值# 这里输出csv文件cat b.txt|awk '&#123;m[$2]++&#125; &#123;s[$2]+=$1&#125; ; END &#123;for(i in m) &#123;print s[i]/m[i] "," m[i] "," i&#125;&#125;'|awk -F "," '$2 &gt; 20'|sort -k2nr &gt; test.csv]]></content>
      <categories>
        <category>linux</category>
        <category>awk</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql简单记录]]></title>
    <url>%2F2019%2F10%2F10%2F11-mysql%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[简单记录一些mysql知识点 SQL 语句主要可以划分为以下 3 个类别123DDL（Data Definition Languages）语句：数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。DML（Data Manipulation Language）语句：数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查）DCL（Data Control Language）语句：数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。 清空表123456789删除表信息的方式有两种 :truncate table table_name;delete * from table_name;注 : truncate操作中的table可以省略，delete操作中的*可以省略truncate、delete 清空表数据的区别 :1&gt; truncate 是整体删除 (速度较快)，delete是逐条删除 (速度较慢)2&gt; truncate 不写服务器 log，delete 写服务器 log，也就是 truncate 效率比 delete高的原因3&gt; truncate 不激活trigger (触发器)，但是会重置Identity (标识列、自增字段)，相当于自增列会被置为初始值，又重新从1开始记录，而不是接着原来的 ID数。而 delete 删除以后，identity 依旧是接着被删除的最近的那一条记录ID加1后进行记录。如果只需删除表中的部分记录，只能使用 DELETE语句配合 where条件 备份12345# 全量锁表备份(不可写)mysqldump --lock-all-tables --all-databases &gt; ALLDB.sql# 仅导出所有表的结构mysqldump --opt -d 数据库名 -u root -p &gt; xxx.sql slave 中修改master_host12345678910# 查看 master.info中信息# 查看 show slave status\G 中 Master_Host# 修改的步骤需要先停止slave1 stop slave ;2 change master to master_host='xxx.xxx.xxx'; 首次配置主库: CHANGE MASTER TO MASTER_HOST='a_master.b.com',MASTER_PORT=3306,MASTER_USER='repl_user',MASTER_PASSWORD='xxxx',MASTER_LOG_FILE='m1-master-bin.000001',MASTER_LOG_POS=88;3 start slave ; mysql问题: navicat连接数据库很慢123456报错: 2013-Lost connection to MYSQL server at 'reading for initial communication packet'说明: 只有windows 的navicat会出现上面报错, windows上通过mysql命令连接时 也很慢#添加如下内容:[mysqld]skip-name-resolve mysql问题: mysql5.7 错误总结-ERROR 1067 (42000): Invalid default value for TIMESTAMP123456show variables like 'sql_mode';+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+| sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ 这是因为sql_mode中的NO_ZEROR_DATE导制的，在strict mode中不允许’0000-00-00’作为合法日期 将上面的NO_ZERO_DATE改为下面的 ALLOW_INVALID_DATES 12set sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';set session sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; 上面的设置是临时设置，在重新登陆后，该设置又恢复为NO_ZERO_DATE mysql5.5主+mysql5.7从 问题123ERROR 1794 (HY000): Slave is not configured or failed to initialize properly. You must at least set --server-id to enable either a master or a slave. Additional error messages can be found in the MySQL error log.server_uuid是5.6的gtid特性引入的一个配置，把mysql5.7的 rpl_slave.cc文件中get_master_uuid函数换成5.6对应的函数就可以了。 mysql一些info信息统计123456789101112131415161718#tbl_size.sqluse information_schema;SELECT TABLE_NAME, ENGINE, ROUND((DATA_LENGTH/1024/1024),2) as DataM , ROUND((INDEX_LENGTH/1024/1024),2) as IndexM, ROUND(((DATA_LENGTH+INDEX_LENGTH)/1024/1024),2) as AllM, TABLE_ROWS, TABLE_COMMENTFROM TABLESWHERE TABLE_SCHEMA = 'hzkj_zh'ORDER BY AllM DESC;# 生成excel表格mysql test &lt;tbl_size.sql &gt;tbl_info_20191028.txt 跳过主从同步错误1234stop slave;SET GLOBAL sql_slave_skip_counter =1;start slave;show slave status\G; mysql information_schema.TABLES表中的table_rows 字段值与’count(*)’ 值不同 查看information_schema 123456789101112131415use information_schema;SELECT TABLE_NAME, TABLE_ROWSFROM TABLESWHERE TABLE_SCHEMA = 'zz' and TABLE_NAME = 'zzz';+---------------------+------------+| TABLE_NAME | TABLE_ROWS |+---------------------+------------+| zzz | 42411396 |+---------------------+------------+ 但是会发现和 1" select count(*) from 某张表; " 执行得到的值是不相同的！那是因为： 1: 默认情况下 mysql 对表进行增删操作时，是不会自动更新 information_schema 库中 tables 表的 table_rows 字段的，在网上搜索一下发现说：只有10%的行数发生变化才会自动收集（没有亲自验证过！）； 2: 执行 Analyze table tableName; 会统计所有表数据（在生产环境中不建议使用，因为会锁表！）；原文链接：mysql information_schema.TABLES表中的table_rows 字段值与count值不同]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6安装nginx1.16+php7.2]]></title>
    <url>%2F2019%2F09%2F27%2F10-centos6%E5%AE%89%E8%A3%85nginx1-16-php7-2%2F</url>
    <content type="text"><![CDATA[记录简单的安装nginx和php的配置,仅供参考 先准备环境123456789# 更新源mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakcurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoyum clean allyum makecacheyum install -y epel-release# 安装一些依赖包yum -y install gcc make cmake ncurses-devel libxml2-devel libtool-ltdl-devel gcc-c++ autoconf automake bison zlib-devel openssl-devel pcre-devel libxml2 libxml2-devel libcurl libcurl-devel autoconf automake libtool-ltdl libtool-ltdl-devel libjpeg libjpeg-turbo-devel libmcrypt-devel libpng-devel centos6编译安装nginx123456789101112131415#首先官网下载1.16cd /usr/local/src/wget http://nginx.org/download/nginx-1.16.0.tar.gztar -xvf nginx-1.16.0.tar.gz# 编译简单的模块./configure --prefix=/usr/local/nginx/ --with-http_ssl_module --with-http_stub_status_module --with-http_stub_status_modulemake -j4make install# 通过启动脚本启动chmod +x /etc/init.d/nginxservice nginx start# 开机启动chkconfig nginx on nginx 启动脚本 /etc/init.d/nginx12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#!/bin/bash# nginx Startup script for the Nginx HTTP Server# it is v.0.0.2 version.# chkconfig: - 85 15# description: Nginx is a high-performance web and proxy server.# It has a lot of features, but it's not for everyone.# processname: nginx# pidfile: /var/run/nginx.pid# config: /usr/local/nginx/conf/nginx.confnginx=/usr/local/nginx/sbin/nginxnginx_config=/usr/local/nginx/conf/nginx.confnginx_pid=/var/run/nginx.pidRETVAL=0prog="nginx"# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ $&#123;NETWORKING&#125; = "no" ] &amp;&amp; exit 0[ -x $nginx ] || exit 0# Start nginx daemons functions.start() &#123;if [ -e $nginx_pid ];then echo "nginx already running...." exit 1fi echo -n $"Starting $prog: " daemon $nginx -c $&#123;nginx_config&#125; RETVAL=$? echo [ $RETVAL = 0 ] &amp;&amp; touch /var/lock/subsys/nginx return $RETVAL&#125;# Stop nginx daemons functions.stop() &#123; echo -n $"Stopping $prog: " killproc $nginx RETVAL=$? echo [ $RETVAL = 0 ] &amp;&amp; rm -f /var/lock/subsys/nginx /usr/local//nginx/logs/nginx.pid&#125;reload() &#123; echo -n $"Reloading $prog: " #kill -HUP `cat $&#123;nginx_pid&#125;` killproc $nginx -HUP RETVAL=$? echo&#125;# See how we were called.case "$1" instart) start ;;stop) stop ;;reload) reload ;;restart) stop start ;;status) status $prog RETVAL=$? ;;*) echo $"Usage: $prog &#123;start|stop|restart|reload|status|help&#125;" exit 1esacexit $RETVAL nginx.conf简单配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697user nobody nobody;worker_processes auto;worker_rlimit_nofile 102400;pid /var/run/nginx.pid;events &#123; use epoll; worker_connections 102400;&#125;http &#123; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; ###定义一个log_format log_format main '[ $host $request_time $upstream_addr $upstream_response_time ] ' '$status ' '$remote_addr - $remote_user [$time_local] "$request" ' '$body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for " "$bytes_sent" " $request_body"' ; ###日志目录配置 #日志全部写入/nginx_logs/access.log 文件中。关闭最后两个server_name的日志 access_log /data/nginx_logs/access.log server_name_main; error_log /data/nginx_logs/error.log notice; ###杂项配置 charset utf-8; #server name的hash表， server_names_hash_bucket_size 128; #请求头如果过小，那么会引起400错误。一般如果cookie过大，会引起问题。getconf PAGESIZE系统分页 client_header_buffer_size 8k; client_body_buffer_size 512k; large_client_header_buffers 16 16k; client_max_body_size 30m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; #fastcgi通用配置 fastcgi_connect_timeout 600; fastcgi_send_timeout 600; fastcgi_read_timeout 600; fastcgi_buffer_size 128k; fastcgi_buffers 8 256k; fastcgi_busy_buffers_size 256k; fastcgi_temp_file_write_size 256k; ###代理有关的配置 proxy_connect_timeout 600; proxy_read_timeout 600; proxy_send_timeout 600; proxy_buffer_size 512k; proxy_buffers 6 512k; proxy_busy_buffers_size 512k; proxy_temp_file_write_size 512k; #或许在于测试,代理服务器不主动关闭客户端，防止499错误 proxy_ignore_client_abort on; ###gzip配置 gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml; gzip_vary on; include /usr/local/nginx/conf/mime.types; default_type application/octet-stream; # 隐藏nginx版本信息 server_tokens off; server &#123; listen 80 default_server; server_name _; #server_name localhost; index index.html index.htm; root html; deny all; location / &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; centos6编译安装php712345678910111213141516# 首先安装freetype2.4tar -xvf freetype-2.4.0.tar.gzcd freetype-2.4.0./configure --prefix=/usr/local/freetypemake -j4 &amp;&amp; make install# 编译php7(不需要的可以去掉)tar -xvf php-7.2.2.tar.gzcd php-7.2.2./configure --prefix=/usr/local/php --with-libxml-dir=/usr/ --with-pdo-mysql=mysqlnd --with-zlib --with-libxml-dir --with-openssl --enable-mysqlnd --enable-mbstring --with-config-file-path=/usr/local/php/etc/ --with-config-file-scan-dir=/usr/local/php/etc/conf.d --enable-fpm --with-freetype-dir=/usr/local/freetype --with-jpeg-dir --with-png-dir --with-gd --enable-gd-native-ttf --enable-pdo --enable-mbstring --enable-bcmathmake -j 4 &amp;&amp; make install# 安装composercurl -sS https://getcomposer.org/installer | php &amp;&amp; mv composer.phar /usr/bin/composer 配置1234567891011# php.ini配置(具体配置内容自行修改)cp php.ini-production /usr/local/php7/etc/php.inicp /usr/local/php7/etc/php-fpm.conf.default /usr/local/php7/etc/php-fpm.confcp /usr/local/php7/etc/php-fpm.d/www.conf.default /usr/local/php7/etc/php-fpm.d/www.conf# 启动脚本cp ./sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod +x /etc/init.d/php-fpm# 创建linkln -s /usr/local/php7 /usr/local/php 启动php-fpm12service php-fpm startchkconfig php-fpm on]]></content>
      <categories>
        <category>技术文档</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>php7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux遇到一些问题统计总结]]></title>
    <url>%2F2019%2F09%2F26%2F9-linux%E9%81%87%E5%88%B0%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[记录一些Linux,nginx或其他服务一些问题 Linux问题: 升级内核123456789101112131415rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm# 查看可升级的内核yum --disablerepo="*" --enablerepo="elrepo-kernel" list availableyum --enablerepo=elrepo-kernel install kernel-ml# 查看已经安装的内核cat /boot/grub2/grub.cfg |grep menuentry# 设置5.3的为默认grub2-set-default 'CentOS Linux (5.3.13-1.el7.elrepo.x86_64) 7 (Core)'# grub2-editenv listsaved_entry=CentOS Linux (5.3.13-1.el7.elrepo.x86_64) 7 (Core) linux问题: tcpdump抓包tcp第三次握手ack为1 执行命令监听: tcpdump -n port 80 (想要详细信息加 -vv) 客户端 telnet x.x.x.x 80 日志如下: 123456tcpdump -n port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on enp0s3, link-type EN10MB (Ethernet), capture size 262144 bytes11:16:40.689157 IP 172.16.54.141.53444 &gt; 172.16.53.106.http: Flags [SEW], seq 1306124348, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 458678777 ecr 0,sackOK,eol], length 011:16:40.689724 IP 172.16.53.106.http &gt; 172.16.54.141.53444: Flags [S.E], seq 1553518959, ack 1306124349, win 64308, options [mss 1410,sackOK,TS val 4208119240 ecr 458678777,nop,wscale 7], length 011:16:40.690320 IP 172.16.54.141.53444 &gt; 172.16.53.106.http: Flags [.], ack 1, win 4106, options [nop,nop,TS val 458678778 ecr 4208119240], length 0 这里第一和第二次握手都没有问题, 第三次 ack 1, 并非是seq+1 这里提一下ACK, ACK 是确认值, ack 是确认编号, 第一次握手ACK=0,在第二次握手开始ACK=1, 而ack是=seq+1(收到的随机数+1) 那么这里ack 1 是啥呢? … 应该就是默认tcpdump 显示成相对值了, 通过-S 参数会显示绝对值 执行命令监听: tcpdump -S -n port 8012345tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on enp0s3, link-type EN10MB (Ethernet), capture size 262144 bytes11:16:54.806628 IP 172.16.54.141.53516 &gt; 172.16.53.106.http: Flags [S], seq 316359286, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 458692791 ecr 0,sackOK,eol], length 011:16:54.806861 IP 172.16.53.106.http &gt; 172.16.54.141.53516: Flags [S.], seq 1113466641, ack 316359287, win 64308, options [mss 1410,sackOK,TS val 4208133357 ecr 458692791,nop,wscale 7], length 011:16:54.807576 IP 172.16.54.141.53516 &gt; 172.16.53.106.http: Flags [.], ack 1113466642, win 4106, options [nop,nop,TS val 458692792 ecr 4208133357], length 0 三次握手图 四次挥手图 linux问题: 禁ping12345678# 一次性修改echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all# 开机自动修改echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all# 永久禁用,加入到/etc/sysctl.confnet.ipv4.icmp_echo_ignore_all=1 linux问题: 文件锁问题1234567问题描述: php slowlog 出现session_start() 慢问题原因: 我们这边有A 和B 两个二级域名,A 会请求 B, 并且由于测试环境在同一台服务器,公用一个php,所以在发生调用的时候同时写了session,而php的sessions配置是默认的file方式, 这就造成了锁的问题问题解决: 1. 修改代码部分2. php的session配置改成redis session.save_handler = redis session.save_path = "tcp://x.x.x.x:xxxx" linux问题: 内存释放问题123456问题描述: 开发这边写了个统计脚本, 占用49G内存, 从日志发现脚本已经全部执行完成, 但是php脚本依然存在问题原因: 通过 strace -p pid 观察进程, 发现是持续性的做内存释放操作 munmap(0x7f6db77ad000, 266240) = 0持续执行munmap函数是因为一直在释放内存(毕竟49G), 结果 =0 说明内存释放执行函数是返回正常了问题解决: 1. 修改代码降低内存2. 等待一段时间内存会释放完成(测试80分钟释放完毕) nginx问题: 静态文件分离对于一般的nginx+php的方式, 我们php采用nobody用户,而代码/lumen采用web-www用户, 这样的好处是页面访问到/lumen时是nobody用户, 是无法修改代码的 可能我们需求是上用户upload图片等, 这时候就可能被传上某个a.php, 这就有可能被代码注入(一般来说图片是放cdn,配置单独域名回源的,这里是直接存在项目目录) 所以为了防止代码注入,我们需要限制upload目录的访问权限 1234# nginx配置如下 location ~ /images/.*\.(gif|jpg|jpeg|png)$ &#123; root /lumen/storage/uploads/; &#125; 这样我们图片传到 /lumen/storage/uploads/images/ 目录, 访问是 www.xxx.com/images/x.png 来访问 且不允许其他类型文件访问. nginx问题: root 和alias在配置文件映射的时候，如果使用了正则表达式，那么可能会出现无法访问文件，nginx可能会将所有的文件都映射成为文件夹，导致文件映射失败的情况出现； root的例子 1234location /a/ &#123; root /lumen/public;&#125;这里实际访问的路径: www.xxx.com/a/ -&gt; /lumen/public/a/ alias的例子 12345# 注意这里目录最后加上/location /a/ &#123; alias /lumen/public/;&#125;这里实际访问的路径: www.xxx.com/a/ -&gt; /lumen/public/ nginx问题: 隐藏版本信息123Syntax: server_tokens on | off | build | string;Default: server_tokens on;Context: http, server, location nginx问题: 日志出现encode内容如何查看12# python2 执行decode&gt;&gt;&gt; print "\x22content\x22\x0D\x0A\x0D\x0A\xE8\x8A\x8A\xE8\x8A\x8A\xE8\xBF\x98\xE6\x80\x95\xE5\xA6\x9E\xE5\xA6\x9E\xE4\xB8\x8D\xE8\x80\x81\xE5\xAE\x9E\xEF\xBC\x8C\xE7\x89\xB9\xE5\x9C\xB0\xE8\xBF\x87\xE6\x9D\xA5\xE8\xA7\x86\xE5\xAF\x9F\xE4\xB8\x80\xE4\xB8\x8B\x0D\x0A".decode('utf-8') nginx问题: default配置未设置nginx 未设置default时, 如果直接访问服务器外网ip, 会去请求到第一个匹配的server段, 有可能会请求到后端的服务器的内容, 这很有可能暴露我们不想暴露的服务一般来说开头添加如下配置 123456 server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; deny all;&#125; nginx 配置已经配置域名方式访问, 如果访问ip会返回403, 正常来说返回403已经不会对服务器造成压力了 可是万万没想到虽然返回了403, 但是也有700字节大小, 大量请求对小带宽来说还是有压力的 123456789101112# server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; return 499; &#125;#其他location,if配置,if ($host != a.example.com) &#123; return 499;&#125; nginx1.11以前trace_id 生成问题 如果是前端(upstream) 123456789101112131415161718log_format server_name_main '"$request_trace_id" [ $host $request_time ] ' '[ $upstream_addr $upstream_response_time ] ' '$status ' '$remote_addr - $remote_user [$time_local] "$request" ' '$body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for" "$bytes_sent"' '&#123;$request_body&#125;' ;...server &#123; set $request_trace_id $pid$connection$bytes_sent$msec; if ( $http_x_request_id != "" )&#123; set $request_trace_id $http_x_request_id; &#125; add_header Bq_F_Traceid $request_trace_id; # 一定要写到location中, 因为proxy_pass location / &#123; proxy_pass http://xxxx; proxy_set_header X-Request-Id $request_trace_id; &#125;&#125; 如果是后端节点 12345678910111213141516171819# 一定要写到server段, 否则后端可能报404错误server &#123; listen 80; server_name openapi-community-alpha.boqii.com ; set $request_trace_id trace-id-$pid-$connection-$bytes_sent-$msec; # 如果请求头中已有该参数,则获取即可;如果没有,则使用$request_id进行填充 set $temp_request_id $http_x_request_id; if ($temp_request_id = "") &#123; set $temp_request_id $request_trace_id; &#125; # 屏蔽掉原来的请求头参数 # proxy_set_header x_request_id ""; proxy_set_header X-Request-Id ""; # 设置向后转发的请求头参数 proxy_set_header X-Request-Id $temp_request_id; location / &#123; try_files $uri $uri/ /index.php?$query_string; &#125;&#125; 修改swap123456789dd if=/dev/zero of=/data/swapfilenew bs=4096 count=4096000swapoff -a /sbin/mkswap /data/swapfilenew/sbin/swapon /data/swapfilenewvim /etc/fstab/data/swapfilenew none swap defaults 0 0]]></content>
      <categories>
        <category>技术文档</category>
        <category>linux</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.7二进制部署]]></title>
    <url>%2F2019%2F09%2F26%2F8-mysql5-7%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[二进制方式部署mysql5.7 下载glibc二进制包12345#打开下载页面, 可能会有小版本更新(注意：选择操作系统时选Linux-Generic）https://dev.mysql.com/downloads/mysql/5.7.html#downloads# 最新的可能有小版本变化wget https://cdn.mysql.com/Downloads/MySQL-5.7/mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz 安装配置1234567891011121314151617181920212223242526272829303132333435363738394041tar -xvf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gzmv mysql-5.7.24-linux-glibc2.12-x86_64 /usr/local/cd /usr/local/# 我的镜像是安装过5.5mysql, 所以需要mv一下mv mysql mysql-5.5.37# 由于以前安装过php指定了该mysq目录, 这可能导致以前安装的php缺少libmysqlclient.so.18ln -s /usr/local/mysql-5.5.37/lib/libmysqlclient.so.18 /usr/local/mysql-5.7.24-linux-glibc2.12-x86_64/lib/libmysqlclient.so.18ln -s mysql-5.7.24-linux-glibc2.12-x86_64 mysql# 添加启动文件\cp mysql/support-files/mysql.server /etc/init.d/mysqldecho "PATH=$PATH:/usr/local/mysql/bin/" &gt;&gt;~/.bashrc# 可选wget http://centos.mirrors.ucloud.cn/centos/6/os/x86_64/Packages/numactl-2.0.9-2.el6.x86_64.rpmyum localinstall numactl-2.0.9-2.el6.x86_64.rpm\rm numactl-2.0.9-2.el6.x86_64.rpmuseradd mysql# 配置下mysql的数据目录cd /data/mkdir u01mkdir u02chown -R mysql.mysql u01chown -R mysql.mysql u02chmod 750 u01chmod 750 u02cd /data/u01/# 初始化/usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/u01cat auto.cnf# 启动服务 (在这之前准备好 /etc/my.cnf)/etc/init.d/mysqld start# 记录下variablesmysql -e "show global variables" &gt;mysql_option_default.log my.cnf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172[client]port = 3306socket = /data/u01/mysql.sock[mysql]prompt="\u@m1-u [\d]&gt; "no-auto-rehash[mysqld]user = mysqlport = 3306basedir = /usr/local/mysqldatadir = /data/u01socket = /data/u01/mysql.sockpid-file = /data/u01/m1-u.pidtmpdir = /data/u02server-id = 1001character-set-server = utf8skip_name_resolve = 1innodb_file_per_table = 1explicit_defaults_for_timestamp = 0# buffer&amp;cachetable_open_cache = 100table_definition_cache = 400table_open_cache_instances = 64sort_buffer_size = 4Mjoin_buffer_size = 4Mread_buffer_size = 8Mread_rnd_buffer_size = 4M# thread&amp;connectionthread_stack = 256Kthread_cache_size = 768back_log = 1024max_connections = 3000max_connect_errors = 1000000# temptabletmp_table_size = 32Mmax_heap_table_size = 32M# networkmax_allowed_packet = 32M#lock_wait_timeout = 3600#interactive_timeout = 600#wait_timeout = 600# query cachequery_cache_size = 0query_cache_type = 0# 设置errorlog、slowlog和generallog的时区，默认UTClog_timestamps = SYSTEM# error-loglog_error = /data/u02/mysqld.log# slow-logslow_query_log = 1slow_query_log_file = /data/u02/slow.loglong_query_time = 0.1log_queries_not_using_indexes =1log_throttle_queries_not_using_indexes = 60min_examined_row_limit = 100log_slow_admin_statements = 1log_slow_slave_statements = 1# general log#general-log = 1general_log_file=/data/u02/query.log# binlogbinlog_format = rowbinlog_checksum = 1log-bin = /data/u02/bdm1-binlog-bin-index = /data/u02/bdm1-bin.indexsync_binlog = 0binlog_cache_size = 4Mmax_binlog_cache_size = 2Gmax_binlog_size = 512Mexpire_logs_days = 15# GTIDgtid_mode = onenforce_gtid_consistency = 1log_slave_updates# Replicationmaster_info_repository = TABLErelay_log_info_repository = TABLEslave-rows-search-algorithms = 'INDEX_SCAN,HASH_SCAN'relay_log_recovery = 1relay_log_purge = 1relay-log=/data/u02/bdm1-relay-binrelay-log-index=/data/u02/bdm1-relay-bin.index# innodb-buffer&amp;cacheinnodb_buffer_pool_size = 2Ginnodb_buffer_pool_instances = 4#innodb_additional_mem_pool_size = 16Minnodb_max_dirty_pages_pct = 50# innodb loginnodb_data_file_path = ibdata1:1G:autoextendinnodb_log_file_size = 1Ginnodb_log_files_in_group = 2innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 32M#innodb_max_undo_log_size = 4G#innodb_undo_directory = undologinnodb_undo_tablespaces = 4# innodb-ioinnodb_flush_method = O_DIRECTinnodb_io_capacity = 600innodb_io_capacity_max = 2000innodb_flush_sync = 0innodb_flush_neighbors = 0#innodb_lru_scan_depth = 4000innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_purge_threads = 4innodb_page_cleaners = 4# transaction,lock#innodb_sync_spin_loops = 100#innodb_spin_wait_delay = 30innodb_lock_wait_timeout = 10innodb_print_all_deadlocks = 1innodb_rollback_on_timeout = 1innodb_open_files = 65535innodb_online_alter_log_max_size = 2G# innodb statusinnodb_status_file = 1# 注意: 开启 innodb_status_output &amp; innodb_status_output_locks 后, 可能会导致log-error文件增长较快innodb_status_output = 0innodb_status_output_locks = 0#performance_schemaperformance_schema = 1performance_schema_instrument = '%=on'#innodb monitorinnodb_monitor_enable="module_innodb"innodb_monitor_enable="module_server"innodb_monitor_enable="module_dml"innodb_monitor_enable="module_ddl"innodb_monitor_enable="module_trx"innodb_monitor_enable="module_os"innodb_monitor_enable="module_purge"innodb_monitor_enable="module_log"innodb_monitor_enable="module_lock"innodb_monitor_enable="module_buffer"innodb_monitor_enable="module_index"innodb_monitor_enable="module_ibuf_system"innodb_monitor_enable="module_buffer_page"innodb_monitor_enable="module_adaptive_hash"# MyISAMkey_buffer_size = 1024Mbulk_insert_buffer_size = 64Mmyisam_sort_buffer_size = 128Mmyisam_repair_threads = 1[mysqldump]quickmax_allowed_packet = 32M]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s部署storageclass动态创建pv(nfs&rbd)]]></title>
    <url>%2F2019%2F09%2F26%2F7-k8s%E9%83%A8%E7%BD%B2storageclass%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv-nfs-rbd%2F</url>
    <content type="text"><![CDATA[考虑到k8s存储的问题, 本机目录挂载存在太大局限性, 多node多pod的服务存储急迫需要共享存储, 这里简单应用k8s storageclass nfs和rbd存储 第一部分 nfs这里单节点简单配置nfs(高并发可采用nfs+rsync+inotify或Sersync) 高并发参考 NFS高可用(NFS+keepalive+Sersync)inotify+rsync实时备份总结 12345678910#安装nfsyum install -y nfs-utils rpcbind# 创建目录mkdir /data/nfsecho "/data/nfs 172.16.76.0/24(rw,sync,no_root_squash) " &gt;&gt;/etc/exports# 启动服务systemctl start rpcbindsystemctl start nfs k8s部署storageclass环境-nfs导入外部配置12345git clone https://github.com/kubernetes-incubator/external-storage.gitcd external-storage/nfs-client/deploy#注意1 node节点需要安装nfs-utils(centos7),nfs-common(ubuntu) 修改deployment.yaml12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: nfs.com/nfs - name: NFS_SERVER value: 172.16.76.134 - name: NFS_PATH value: /data/nfs volumes: - name: nfs-client-root nfs: server: 172.16.76.134 path: /data/nfs 修改class.yaml1234567apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfsprovisioner: nfs.com/nfsparameters: archiveOnDelete: "false" rbac.yaml不用修改1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859rbac.yamlkind: ServiceAccountapiVersion: v1metadata: name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionerrules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 部署nfs环境(创建nfs存储类)1kubectl apply -f rbac.yaml -f class.yaml -f deployment.yaml k8s中部署nginx项目采用nfs存储部署nginx-deployment-nfs.yaml(测试nfs) 这种方式会创建一个pvc 挂载到多个pod中,这种方式适合nginx-html挂载 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465---kind: PersistentVolumeClaimapiVersion: v1metadata: name: html0-deploy-nfs annotations: volume.beta.kubernetes.io/storage-class: 'nfs'spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx0-deployspec: replicas: 2 template: metadata: labels: app: nginx0-deploy spec: containers: - name: nginx0-deploy image: hub.boqii.com/bq/nginx:1.15.12 ports: - containerPort: 80 volumeMounts: - name: html0-deploy-nfs mountPath: /usr/share/nginx/html - name: nginx-config mountPath: "/etc/nginx/conf.d" volumes: - name: nginx-config configMap: name: nginx-config - name: html0-deploy-nfs persistentVolumeClaim: claimName: html0-deploy-nfs---apiVersion: v1kind: ConfigMapmetadata: name: nginx-configdata: default.conf: | server &#123; listen 80; server_name localhost; root /usr/share/nginx/html/; access_log /var/log/nginx/host_access.log; error_log /var/log/nginx/host_error.log debug; location / &#123; root /usr/share/nginx/html/; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; &#125; 部署nginx-statefulset-nfs.yaml(测试nfs) 这里statefulset 方式会创建多个pvc, 每个pod的html就可以都不一样! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758---apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: nginx3spec: serviceName: "nginx" replicas: 2 volumeClaimTemplates: - metadata: name: html annotations: volume.beta.kubernetes.io/storage-class: "nfs" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 2Gi template: metadata: labels: app: nginx spec: containers: - name: nginx image: hub.boqii.com/bq/nginx:1.15.12 volumeMounts: - mountPath: "/usr/share/nginx/html/" name: html - mountPath: "/etc/nginx/conf.d" name: nginx-config volumes: - name: nginx-config configMap: name: nginx-config---apiVersion: v1kind: ConfigMapmetadata: name: nginx-configdata: default.conf: | server &#123; listen 80; server_name localhost; root /usr/share/nginx/html/; access_log /var/log/nginx/host_access.log; error_log /var/log/nginx/host_error.log debug; location / &#123; root /usr/share/nginx/html/; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; &#125; 另外说明一下将nfs作为文件存储类似mount方式,这种方式不适用于多容器自动化部署 ,显然这种并不适合ceph rbd存储, cephfs是可以的首先需要在nfs目录创建需要挂载的目录12#例如mkdir -p /data/nfs/k8s-db-t/mysql-data-dev 在部署的yml中直接mount nfs的目录1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: mysql-server namespace: devopsspec: replicas: 1 template: metadata: labels: app: mysql-server spec: containers: - image: mysql:5.7.16 imagePullPolicy: Always name: mysql-server ports: - containerPort: 3306 protocol: TCP volumeMounts: - name: mysql-data mountPath: /var/lib/mysql resources: requests: cpu: 40m memory: 32Mi limits: cpu: "300m" memory: 256Mi env: - name: MYSQL_ROOT_PASSWORD value: "boqii.123" - name: MYSQL_DATABASE value: "gogs" - name: MYSQL_USER value: "gogs" - name: MYSQL_PASSWORD value: "gogspass" - name: TZ value: "Asia/Shanghai" volumes: - name: mysql-data nfs: server: 172.16.76.134 path: /data/nfs/k8s-db-t/mysql-data-dev---apiVersion: v1kind: Servicemetadata: name: mysql-service namespace: devopsspec: clusterIP: None selector: app: mysql-server ports: - name: http port: 3306 第二部分 cephk8s部署storageclass环境-ceph如果集群是用kubeadm部署的，由于controller-manager官方镜像中没有rbd命令，所以我们要导入外部配置12git clone https://github.com/kubernetes-incubator/external-storage.gitcd external-storage/ceph/rbd/deploy 以下整合在一个文件, 两个版本,默认 和retain storageclass-cepm.com-rbd.yaml123456789101112131415161718192021222324252627282930313233343536---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admintype: kubernetes.io/rbd---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admin namespace: ns-elastictype: kubernetes.io/rbd---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rbd annotations: storageclass.kubernetes.io/is-default-class: "true"provisioner: ceph.com/rbdparameters: monitors: 172.16.76.134:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: storageclass-rbd userId: admin userSecretName: ceph-secret-admin fsType: ext4 imageFormat: "2" imageFeatures: "layering" storageclass-cepm.com-rbd-retain.yaml12345678910111213141516171819202122232425262728293031323334353637---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admintype: kubernetes.io/rbd---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admin namespace: ns-elastictype: kubernetes.io/rbd---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rbd-retain annotations: storageclass.kubernetes.io/is-default-class: "false"provisioner: ceph.com/rbdreclaimPolicy: Retainparameters: monitors: 172.16.76.134:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: storageclass-rbd-retain userId: admin userSecretName: ceph-secret-admin fsType: ext4 imageFormat: "2" imageFeatures: "layering" k8s中部署nginx项目采用 ceph.com/rbd 和nfs类似, 这里省略 以上采用的是persistentVolumeClaim 方式动态分配全部内容]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>存储</category>
        <category>ceph</category>
        <category>storageclass</category>
        <category>nfs</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ceph</tag>
        <tag>k8s存储</tag>
        <tag>nfs</tag>
        <tag>storageclass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph安装部署]]></title>
    <url>%2F2019%2F09%2F26%2F6-ceph%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。 简单了解什么是块存储/对象存储/文件系统存储？ceph 目前提供对象存储（RADOSGW）、块存储RDB以及 CephFS 文件系统这 3 种功能。对于这3种功能介绍，分别如下： 对象存储，也就是通常意义的键值存储，其接口就是简单的GET、PUT、DEL 和其他扩展，代表主要有 Swift 、S3 以及 Gluster 等； 块存储，这种接口通常以 QEMU Driver 或者 Kernel Module 的方式存在，这种接口需要实现 Linux 的 Block Device 的接口或者 QEMU 提供的 Block Driver 接口，如 Sheepdog，AWS 的 EBS，青云的云硬盘和阿里云的盘古系统，还有 Ceph 的 RBD（RBD是Ceph面向块存储的接口）。在常见的存储中 DAS、SAN 提供的也是块存储； 文件存储，通常意义是支持 POSIX 接口，它跟传统的文件系统如 Ext4 是一个类型的，但区别在于分布式存储提供了并行化的能力，如 Ceph 的 CephFS (CephFS是Ceph面向文件存储的接口)，但是有时候又会把 GlusterFS ，HDFS 这种非POSIX接口的类文件存储接口归入此类。当然 NFS、NAS也是属于文件系统存储； 参考教程Kubernetes 集成 Ceph 后端存储教程centos7安装ceph集群 准备配置源12345678910111213141516171819202122cat &gt;/etc/yum.repos.d/ceph.repo&lt;&lt;EOF[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMSenabled=0gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1EOF 安装过卸载123ceph-deploy purge dk1-t dk2-tceph-deploy purgedata dk1-t dk2-tceph-deploy forgetkeys 在dk2-t节点创建集群 mon模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647yum install ceph-deploy -yceph-deploy --versionmkdir /data/cephcd /data/cephceph-deploy new dk2-t# 查看配置文件ls -l# 配置ceph.conf[global]...# 如果有多个网卡，应该配置如下选项，# public network是公共网络，负责集群对外提供服务的流量# cluster network是集群网络，负载集群中数据复制传输通信等# 本次实验使用同一块网卡，生境环境建议分别使用一块网卡public network = 172.16.76.0/22cluster network = 172.16.76.0/22osd pool default size = 2# 安装 ceph 包# 如果按照官方文档安装方法 会重新配置安装官方ceph源# 由于网络问题，安装可能会出错，需要多次执行# ceph-deploy install 其实只是会安装 ceph ceph-radosgw 两个包# ceph-deploy install lab1 lab2 lab3# 推荐使用阿里源安装，因为使用ceph-deploy安装会很慢# 使用如下命令手动安装包，替代官方的 ceph-deploy install 命令# 如下操作在所有node节点上执行export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-luminous/el7export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc# 先执行是因为 ceph-deploy install太慢yum install -y ceph ceph-radosgwceph-deploy install dk2-t# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy dk1-t dk2-t# 额外mon节点，mon节点也需要高可用ceph-deploy mon add dk1-t 在dk2-t节点创建集群 mgr模块123# 部署manager （luminous+）12及以后的版本需要部署# 本次部署 jewel 版本 ，不需要执行如下命令 ceph-deploy mgr create dk2-t 在dk2-t节点创建集群 osd模块1234# 12的版本(这里挂载一个 10G的磁盘 /dev/sdb)# create 命令一次完成准备 OSD 、部署到 OSD 节点、并激活它。 create 命令是依次执行 prepare 和 activate 命令的捷径。ceph-deploy osd create --data /dev/sdb dk2-tceph-deploy osd create --data /dev/sdc dk1-t 如何卸载osd1234567891011121314# 查看ceph osd tree# 节点状态标记为outceph osd out osd.0# 从crush中移除节点ceph osd crush remove osd.0# 删除节点ceph osd rm osd.0# 删除节点认证（不删除编号会占住）ceph auth del osd.0 查看 mon 信息12345678ceph mon dumpdumped monmap epoch 1epoch 1fsid 4620d0c7-4458-4ff9-9296-d1318058bafclast_changed 2019-06-19 14:44:41.361005created 2019-06-19 14:44:41.3610050: 172.16.76.134:6789/0 mon.dk2-t 配置文件内容 /etc/ceph/ceph.conf1234567891011[global]public network = 172.16.76.0/22cluster network = 172.16.76.0/22osd pool default size = 2fsid = 4620d0c7-4458-4ff9-9296-d1318058bafcmon_initial_members = dk2-tmon_host = 172.16.76.134auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_max_pg_per_osd = 1000 ceph 一些测试命令创建 rbd pool，名字叫做 kube1ceph osd pool create kube 256 256 如何取得admin的密钥12ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '&#123;print $3&#125;'AQAn/19bbb21GBAA1kc0HRWoGjeoPTRQziA03A== 测试ceph是否正常12345678910111213rbd create kube/test --size 1024 --image-format 2rbd ls kuberbd map kube/test # 如果报错, 警用 rbd info kube/test rbd feature disable kube/test exclusive-lock object-map fast-diff deep-flattenrbd map kube/testrbd showmappedmkfs.ext4 /dev/rbd0mkdir /data/rbd0mount /dev/rbd0 /data/rbd0cd /data/rbd0 &amp;&amp; echo test &gt; test.txt 在k8s 手动创建存储类创建ceph pg123456789101112131415161718192021# Total PGs = (Total_number_of_OSD * 100) / max_replication_count# pg = 1 * 100 /2 ~ 64(取2的次方数)# 这里准备创建2个pool, 每个poolceph osd pool create rbd-k8s 16# 查看ceph osd lspools# 创建imagerbd create rbd-k8s/cephimageredis --size 500M# 查看listrbd list rbd-k8s# 处理新特性# 查看inforbd info rbd-k8s/cephimageredis# 关闭exclusive-lock object-map fast-diff deep-flatten 这些特性rbd feature disable rbd-k8s/cephimageredis exclusive-lock object-map fast-diff deep-flatten 首先创建secret12#获取keygrep key /etc/ceph/ceph.client.admin.keyring |awk '&#123;printf "%s", $NF&#125;'|base64 ceph-secret.yaml1234567apiVersion: v1kind: Secretmetadata: name: ceph-secrettype: "kubernetes.io/rbd"data: key: QVFCTFo2dGNGNXFLRnhBQXBGTXJEdm5CY2k2UGtwZmZrN0JSVEE9PQ== 其次创建pv, pv是没有namespace概念的 persistentVolumeReclaimPolicy是清理规则 (retain: 不清理, Recycle: 回收) redis-ceph-pv.yml 1234567891011121314151617181920apiVersion: v1kind: PersistentVolumemetadata: name: redis2-ceph-rbd-pvspec: capacity: storage: 100Mi accessModes: - ReadWriteOnce rbd: monitors: - '172.16.76.134:6789' pool: rbd-k8s image: cephimageredis user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Recycle 执行部署pv1kubectl create -f redis-ceph-pv.yml 然后创建pvc redis-ceph-pvc.yml 12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: redis2-ceph-rbd-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 执行部署pvc1kubectl create -f redis-ceph-pvc.yml 最后在rancher上选择挂载rbd]]></content>
      <categories>
        <category>技术文档</category>
        <category>存储</category>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ceph</tag>
        <tag>cephfs</tag>
        <tag>k8s存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加看板娘]]></title>
    <url>%2F2019%2F09%2F24%2F5-hexo%E6%B7%BB%E5%8A%A0%E7%9C%8B%E6%9D%BF%E5%A8%98%2F</url>
    <content type="text"><![CDATA[hexo6 左下角添加看板娘 github地址: 张书樵大神 下载大神项目 (会说话,换人物,小游戏等功能)12cd themes/nextv6/sourcegit clone https://github.com/stevenjoezhang/live2d-widget.git github说明比较详细, 这里简单说明 由于这里是克隆到了source目录, hexo d -g的时候会生成到public目录, 相当于站点根目录了 123456# 直接开启autoload.js注释const live2d_path = "/live2d-widget/";# 修改 themes/nextv6/layout/_layout.swig, 最后一行添加如下&lt;!-- 看板娘 --&gt;&lt;script src="/live2d-widget/autoload.js"&gt;&lt;/script&gt; 一般小白简单教程(只有看鼠标方向功能) hexo 官方支持版 需要安装模板1npm install --save hexo-helper-live2d 修改主题配置文件各种宠物预览 12345678910111213141516171819202122232425# Live2D## https://github.com/EYHN/hexo-helper-live2dlive2d: enable: true # enable: false scriptFrom: local # 默认 pluginRootPath: live2dw/ # 插件在站点上的根目录(相对路径) pluginJsPath: lib/ # 脚本文件相对与插件根目录路径 pluginModelPath: assets/ # 模型文件相对与插件根目录路径 # scriptFrom: jsdelivr # jsdelivr CDN # scriptFrom: unpkg # unpkg CDN # scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 你的自定义 url tagMode: false # 标签模式, 是否仅替换 live2d tag标签而非插入到所有页面中 debug: false # 调试, 是否在控制台输出日志 model: use: live2d-widget-model-haruto # npm-module package name # use: wanko # 博客根目录/live2d_models/ 下的目录名 # use: ./wives/wanko # 相对于博客根目录的路径 # use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 你的自定义 url display: position: left width: 150 height: 300 mobile: show: true # 手机中是否展示]]></content>
      <categories>
        <category>有趣</category>
        <category>博客</category>
        <category>二次元</category>
        <category>美化</category>
      </categories>
      <tags>
        <tag>hexo6</tag>
        <tag>特效</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo鼠标移动和鼠标点击特效]]></title>
    <url>%2F2019%2F09%2F24%2F4-hexo%E9%BC%A0%E6%A0%87%E7%A7%BB%E5%8A%A8%E5%92%8C%E9%BC%A0%E6%A0%87%E7%82%B9%E5%87%BB%E7%89%B9%E6%95%88%2F</url>
    <content type="text"><![CDATA[hexo6 鼠标添加点击出现桃心的特效 来自: Next主题个性化 所需要的js文件 未压缩 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051! function(e, t, a) &#123; function n() &#123; c( ".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"), o(), r() &#125; function r() &#123; for (var e = 0; e &lt; d.length; e++) d[e].alpha &lt;= 0 ? (t.body.removeChild(d[e].el), d.splice(e, 1)) : (d[e].y--, d[e].scale += .004, d[e].alpha -= .013, d[e].el.style.cssText = "left:" + d[e].x + "px;top:" + d[e].y + "px;opacity:" + d[e].alpha + ";transform:scale(" + d[e].scale + "," + d[e].scale + ") rotate(45deg);background:" + d[e].color + ";z-index:99999"); requestAnimationFrame(r) &#125; function o() &#123; var t = "function" == typeof e.onclick &amp;&amp; e.onclick; e.onclick = function(e) &#123; t &amp;&amp; t(), i(e) &#125; &#125; function i(e) &#123; var a = t.createElement("div"); a.className = "heart", d.push(&#123; el: a, x: e.clientX - 5, y: e.clientY - 5, scale: 1, alpha: 1, color: s() &#125;), t.body.appendChild(a) &#125; function c(e) &#123; var a = t.createElement("style"); a.type = "text/css"; try &#123; a.appendChild(t.createTextNode(e)) &#125; catch (t) &#123; a.styleSheet.cssText = e &#125; t.getElementsByTagName("head")[0].appendChild(a) &#125; function s() &#123; return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")" &#125; var d = []; e.requestAnimationFrame = function() &#123; return e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function(e) &#123; setTimeout(e, 1e3 / 60) &#125; &#125;(), n()&#125;(window, document); 压缩后 1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 将上面的内容贴到新增的themes/nextv6/source/js/src/love.js 文件中修改themes/nextv6/layout/_layout.swig文件, 末尾添加如下内容12&lt;!-- 鼠标桃心动画 --&gt;&lt;script type="text/javascript" src="/js/src/love.js"&gt;&lt;/script&gt; hexo6 鼠标移动添加星星特效来自: 愚人节鼠标跟随特效 新增js文件 themes/nextv6/source/js/src/love2.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/*! * Fairy Dust Cursor.js * - 90's cursors collection * -- https://github.com/tholman/90s-cursor-effects * -- http://codepen.io/tholman/full/jWmZxZ/ */(function fairyDustCursor() &#123; var possibleColors = ["#D61C59", "#E7D84B", "#1B8798"] var width = window.innerWidth; var height = window.innerHeight; var cursor = &#123;x: width/2, y: width/2&#125;; var particles = []; function init() &#123; bindEvents(); loop(); &#125; // Bind events that are needed function bindEvents() &#123; document.addEventListener('mousemove', onMouseMove); document.addEventListener('touchmove', onTouchMove); document.addEventListener('touchstart', onTouchMove); window.addEventListener('resize', onWindowResize); &#125; function onWindowResize(e) &#123; width = window.innerWidth; height = window.innerHeight; &#125; function onTouchMove(e) &#123; if( e.touches.length &gt; 0 ) &#123; for( var i = 0; i &lt; e.touches.length; i++ ) &#123; addParticle( e.touches[i].clientX, e.touches[i].clientY, possibleColors[Math.floor(Math.random()*possibleColors.length)]); &#125; &#125; &#125; function onMouseMove(e) &#123; cursor.x = e.clientX; cursor.y = e.clientY; addParticle( cursor.x, cursor.y, possibleColors[Math.floor(Math.random()*possibleColors.length)]); &#125; function addParticle(x, y, color) &#123; var particle = new Particle(); particle.init(x, y, color); particles.push(particle); &#125; function updateParticles() &#123; // Updated for( var i = 0; i &lt; particles.length; i++ ) &#123; particles[i].update(); &#125; // Remove dead particles for( var i = particles.length -1; i &gt;= 0; i-- ) &#123; if( particles[i].lifeSpan &lt; 0 ) &#123; particles[i].die(); particles.splice(i, 1); &#125; &#125; &#125; function loop() &#123; requestAnimationFrame(loop); updateParticles(); &#125; /** * Particles */ function Particle() &#123; this.character = "*"; this.lifeSpan = 120; //ms this.initialStyles =&#123; "position": "fixed", "top": "0", //必须加 "display": "block", "pointerEvents": "none", "z-index": "10000000", "fontSize": "20px", "will-change": "transform" &#125;; // Init, and set properties this.init = function(x, y, color) &#123; this.velocity = &#123; x: (Math.random() &lt; 0.5 ? -1 : 1) * (Math.random() / 2), y: 1 &#125;; this.position = &#123;x: x - 10, y: y - 20&#125;; this.initialStyles.color = color; console.log(color); this.element = document.createElement('span'); this.element.innerHTML = this.character; applyProperties(this.element, this.initialStyles); this.update(); document.body.appendChild(this.element); &#125;; this.update = function() &#123; this.position.x += this.velocity.x; this.position.y += this.velocity.y; this.lifeSpan--; this.element.style.transform = "translate3d(" + this.position.x + "px," + this.position.y + "px,0) scale(" + (this.lifeSpan / 120) + ")"; &#125; this.die = function() &#123; this.element.parentNode.removeChild(this.element); &#125; &#125; /** * Utils */ // Applies css `properties` to an element. function applyProperties( target, properties ) &#123; for( var key in properties ) &#123; target.style[ key ] = properties[ key ]; &#125; &#125; init();&#125;)(); 修改themes/nextv6/layout/_layout.swig文件, 末尾添加如下内容12&lt;!-- 鼠标移动星星特效 --&gt;&lt;script type="text/javascript" src="/js/src/love2.js"&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>有趣</category>
        <category>博客</category>
        <category>美化</category>
      </categories>
      <tags>
        <tag>hexo6</tag>
        <tag>特效</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s遇到的一些问题统计总结]]></title>
    <url>%2F2019%2F09%2F20%2F3-k8s%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[不定时更新,文章可能比较散乱,&gt;_&lt; 1. 单机版k8s pod一直是pending的问题 describe一下pod会发现错误: 1 node(s) had taints that the pod didnt tolerate.这是因为master上存在污点,pod不会再改节点上创建两种办法: deploy 的时候加上 容忍该污点 直接取消master上的污点 12345# 取消master上污点 kubectl taint nodes --all node-role.kubernetes.io/master-# 查看taintkubectl describe node node1 2. 修改service-node-port-range 由于traefik部署需要对外开放80端口, 但默认仅允许30000以上端口 123456789# kubeadm 1.14 配置apiServer: extraArgs: authorization-mode: Node,RBAC service-node-port-range: 79-33000# kubeadm 1.10配置apiServerExtraArgs: service-node-port-range: 79-33000 3. traefik断电后重新启动报错 command traefik error: field not found, node: redirect12345看到这个错误猜测可能是用的latest镜像问题, 从hub.docker.com 查看更新了v2.0+的版本将traefik的deployment配置中 image改成 traefik:1.7重新部署后 问题解 4. 查看当前集群的(CustomResourceDefinition)12345678# 查看k8s有哪些apikubectl api-versions# 查看当前crdkubectl get crd# 其次查看该api是什么版本kubectl describe crd destinationrules.networking.istio.io 5. 启用自动轮换kubelet 证书(证书未过期)参考: Kubeadm证书过期时间调整 kubelet证书分为server和client两种， k8s 1.9默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启 增加 kubelet 参数 123# 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数Environment="KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true" 增加 controller-manager 参数 123456# 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数 - command: - kube-controller-manager - --experimental-cluster-signing-duration=87600h0m0s - --feature-gates=RotateKubeletServerCertificate=true - .... 创建 rbac 对象创建 rbac对象，允许节点轮换kubelet server证书： 1234567891011121314151617181920212223242526272829303132cat &gt; ca-update.yaml &lt;&lt; EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: - certificates.k8s.io resources: - certificatesigningrequests/selfnodeserver verbs: - create---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubeadm:node-autoapprove-certificate-serverroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserversubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodesEOFkubectl create –f ca-update.yaml 重新启动kubelet 123systemctl daemon-reloadsystemctl enable kubeletsystemctl restart kubelet 6. hpa的一个cpu-percent百分比问题1kubectl autoscale deployment php-admintest-nginx-dev --cpu-percent=80 --min=1 --max=2 -n php-dev 例如以上,我希望在平均cpu超过80%时,pod能自动调整为2个 em…这没啥问题 但我做简单ab压测发现, 我把cpu压到了100000% … 我的deployment 配置中是这样限制cpu的 12345resources: requests: cpu: "1m" limits: cpu: "1000m" 显然我的pod可以使用1个核cpu, 那这个平均cpu是等于啥呢? cpu-percent = 1000m/resources.request.cpu =&gt; 1000m/1m =100000% -_-!!! 稍微解释下,为啥我要设置request.cpu=1m:比如单机4核k8s,我启动了1个pod,limit是4cpu,那么我request.cpu其实默认也是1, 所以集群就已经预留了4cpu, 此时如果在启动pod, 在配置limit的时候就无法成功启动pod,因为核心不够了,都给那什么玩意了…(当然这样改动也的确会出现过度分配) 因此建议 修改requests.cpu=500m, –cpu-percent可设范围: 0200% ,或者低一些 250m -&gt; 0400% 总结:request.cpu 必须设置, 这个是对比的对象 另外:对于扩容而言，这个时间段为3分钟，缩容为5分钟(可以通过 –horizontal-pod-autoscaler-downscale-delay ， –horizontal-pod-autoscaler-upscale-delay 进行调整)。 7 k8s1.16.2+metrics v0.3.5 deployment重启之后hpa就失效,无法获取到数据1The HPA was unable to compute the replica count: unable to get metrics for resource cpu: no metrics returned from resource metrics API]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署elk7.2.0]]></title>
    <url>%2F2019%2F09%2F19%2F2-%E9%83%A8%E7%BD%B2elk7-2-0%2F</url>
    <content type="text"><![CDATA[说明: 121 单台k8s,本机目录挂载(未配置cephfs)2 如果replicas大于1, 就会出现多个es挂载同一个目录,会出现报错(uuid block) 1. es配置本地挂载 k8s-es-7.2.0.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128---apiVersion: v1kind: ServiceAccountmetadata: labels: app: elasticsearch name: elasticsearch7-admin namespace: ns-elastic7---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: elasticsearch7-admin labels: app: elasticsearchroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: elasticsearch7-admin namespace: ns-elastic7---apiVersion: apps/v1kind: StatefulSetmetadata: labels: app: elasticsearch role: master name: elasticsearch-master namespace: ns-elastic7spec: replicas: 1 serviceName: elasticsearch-master selector: matchLabels: app: elasticsearch role: master template: metadata: labels: app: elasticsearch role: master spec: serviceAccountName: elasticsearch7-admin restartPolicy: Always securityContext: fsGroup: 1000 containers: - name: elasticsearch-master image: hub.boqii.com/bq/elasticsearch:7.2.0 command: ["bash", "-c", "ulimit -l unlimited &amp;&amp; sysctl -w vm.max_map_count=262144 &amp;&amp; chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data &amp;&amp; exec su elasticsearch docker-entrypoint.sh"] imagePullPolicy: IfNotPresent securityContext: privileged: true ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "800m" env: - name: cluster.name value: "es_cluster" - name: node.master value: "true" - name: node.data value: "true" - name: cluster.initial_master_nodes value: "elasticsearch-master-0" # 根据副本数和name配置 - name: discovery.zen.ping_timeout value: "5s" - name: node.ingest value: "false" - name: ES_JAVA_OPTS value: "-Xms1g -Xmx1g" - name: "discovery.zen.ping.unicast.hosts" value: "elasticsearch-discovery" # Disvocery Service - name: "http.cors.enabled" value: "true" - name: "http.cors.allow-origin" value: "*" volumeMounts: - name: elasticsearch-data-volume mountPath: /usr/share/elasticsearch/data volumes: - name: elasticsearch-data-volume hostPath: path: /data/k8s-container/elk-7.2.0/es-7.2.0/data---apiVersion: v1kind: Servicemetadata: labels: app: elasticsearch name: elasticsearch-discovery namespace: ns-elastic7spec: publishNotReadyAddresses: true ports: - name: transport port: 9300 targetPort: 9300 selector: app: elasticsearch role: master---kind: ServiceapiVersion: v1metadata: labels: app: elasticsearch name: elasticsearch-service namespace: ns-elastic7spec: type: NodePort ports: - port: 9200 targetPort: 9200 nodePort: 19230 protocol: TCP selector: app: elasticsearch 2. es配置nfs动态挂载 k8s-es-7.2.0-nfs.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129---apiVersion: v1kind: ServiceAccountmetadata: labels: app: elasticsearch name: elasticsearch-admin namespace: ns-elastic---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: elasticsearch-admin labels: app: elasticsearchroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: elasticsearch-admin namespace: ns-elastic---apiVersion: apps/v1kind: StatefulSetmetadata: labels: app: elasticsearch role: master name: elasticsearch-master namespace: ns-elasticspec: replicas: 2 volumeClaimTemplates: - metadata: name: elasticsearch-data-nfs annotations: volume.beta.kubernetes.io/storage-class: "nfs" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 2Gi serviceName: elasticsearch-master selector: matchLabels: app: elasticsearch role: master template: metadata: labels: app: elasticsearch role: master spec: serviceAccountName: elasticsearch-admin restartPolicy: Always securityContext: fsGroup: 1000 containers: - name: elasticsearch-master image: elasticsearch:7.2.0 command: ["bash", "-c", "ulimit -l unlimited &amp;&amp; sysctl -w vm.max_map_count=262144 &amp;&amp; chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data &amp;&amp; exec su elasticsearch docker-entrypoint.sh"] imagePullPolicy: IfNotPresent volumeMounts: - name: elasticsearch-data-nfs mountPath: /usr/share/elasticsearch/data securityContext: privileged: true ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: cluster.name value: "es_cluster" - name: node.master value: "true" - name: node.data value: "true" - name: cluster.initial_master_nodes value: "elasticsearch-master-0,elasticsearch-master-1" # 根据副本数和name配置 - name: discovery.zen.ping_timeout value: "5s" - name: node.ingest value: "false" - name: ES_JAVA_OPTS value: "-Xms1g -Xmx1g" - name: "discovery.zen.ping.unicast.hosts" value: "elasticsearch-discovery" # Disvocery Service - name: "http.cors.enabled" value: "true" - name: "http.cors.allow-origin" value: "*"---apiVersion: v1kind: Servicemetadata: labels: app: elasticsearch name: elasticsearch-discovery namespace: ns-elasticspec: publishNotReadyAddresses: true ports: - name: transport port: 9300 targetPort: 9300 selector: app: elasticsearch role: master---kind: ServiceapiVersion: v1metadata: labels: app: elasticsearch name: elasticsearch-service namespace: ns-elasticspec: type: NodePort ports: - port: 9200 targetPort: 9200 nodePort: 19220 protocol: TCP selector: app: elasticsearch 3. kibana配置k8s-kibana-7.2.0.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485apiVersion: v1kind: ConfigMapmetadata: name: kibana-config namespace: ns-elastic7 labels: elastic-app: kibanadata: kibana.yml: | server.name: kibana server.host: "0" elasticsearch.hosts: [ "http://elasticsearch-service:9200" ] xpack.monitoring.ui.container.elasticsearch.enabled: true---kind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: kibana name: kibana namespace: ns-elastic7spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: kibana template: metadata: labels: elastic-app: kibana spec: containers: - name: kibana image: hub.boqii.com/bq/kibana:7.2.0 ports: - containerPort: 5601 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "800m" volumeMounts: - name: kibana-config mountPath: /usr/share/kibana/config volumes: - name: kibana-config configMap: name: kibana-config tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---kind: ServiceapiVersion: v1metadata: labels: elastic-app: kibana name: kibana-service namespace: ns-elastic7spec: ports: - port: 5601 targetPort: 5601 selector: elastic-app: kibana type: NodePort---apiVersion: extensions/v1beta1kind: Ingressmetadata: labels: elastic-app: kibana name: kibana-ingress namespace: ns-elastic7spec: rules: - host: elk-kibana-dev.boqii.com http: paths: - backend: serviceName: kibana-service servicePort: 5601 4. logstash配置 本地挂载 k8s-logstash-7.2.0.yml 4.1 config/pipelines.yml 12- pipeline.id: main path.config: "/usr/share/logstash/config/pipeline/*.conf" 4.2 首先配置grok规则 config/pipeline/logstash.conf 12345678910111213141516171819202122input &#123; udp &#123; port =&gt; "10000" &#125; &#125; filter &#123; grok &#123; match =&gt; &#123; "message" =&gt; "\&#123;\"id\":\"(?&lt;id&gt;(.)*)\",\"tag\":\"(?&lt;tag&gt;(.)*)\",\"title\":\"%&#123;GREEDYDATA:title&#125;(?&lt;title&gt;(.|\r|\n)*)\",\"value\":\"%&#123;GREEDYDATA:value&#125;(?&lt;value&gt;(.|\r|\n)*)\",\"createdAt\":\"(?&lt;createdAt&gt;\S+ \S+)\",\"Telephone\":\"(?&lt;Telephone&gt;(.)*)\",\"uid\":\"(?&lt;uid&gt;(.)*)\",\"updateTime\":\"(?&lt;updateTime&gt;(.)*)\",\"appVersion\":\"(?&lt;appVersion&gt;(.)*)\",\"mobileModel\":\"(?&lt;mobileModel&gt;(.)*)\",\"osVersion\":\"(?&lt;osVersion&gt;(.)*)\",\"channel\":\"(?&lt;channel&gt;(.)*)\",\"UDID\":\"(?&lt;UDID&gt;(.)*)\"\&#125;" &#125; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [ "http://elasticsearch-service:9200" ] index =&gt; "k8s2-dev-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; 4.3 配置文件 k8s-logstash-7.2.0.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455---kind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: logstash name: logstash namespace: ns-elasticspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: logstash template: metadata: labels: elastic-app: logstash spec: containers: - name: logstash image: hub.boqii.com/bq/logstash:7.2.0 ports: - containerPort: 10000 protocol: UDP volumeMounts: - name: logstash-config mountPath: /usr/share/logstash/config volumes: - name: logstash-config hostPath: path: /data/k8s-pod/elk-7.2.0/logstash-7.2.0/config tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---kind: ServiceapiVersion: v1metadata: labels: elastic-app: logstash name: logstash-service namespace: ns-elasticspec: type: NodePort ports: - port: 10000 targetPort: 10000 nodePort: 10000 protocol: UDP selector: elastic-app: logstash type: NodePort---]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>elk</category>
        <category>elk7</category>
        <category>elasticsearch7</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>elk7</tag>
        <tag>elk</tag>
        <tag>elasticsearch7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[首次搭建hexo博客系统]]></title>
    <url>%2F2019%2F09%2F19%2F%E9%A6%96%E6%AC%A1%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1 安装hexo 1.1 在mac上安装 12# 安装nodebrew install node npm 1.2 在linux安装 123# 安装node10curl -sL https://rpm.nodesource.com/setup_10.x | bash -yum install -y nodejs 1.3 安装hexo 12# 安装hexonpm install -g hexo 2 初始化123456789101112cd /data/github/# 初始化hexo init blog# 框架安装npm install#安装 Hexo 关于启动服务器的插件npm install hexo-server --save# 启动服务器, 本地查看效果, 如果不指定端口，默认为4000hexo server 3 主题和配置 下载主题：https://github.com/theme-next/hexo-theme-next 12unzip hexo-theme-next-master.zipmv hexo-theme-next-master $blog/themes/ 修改主题配置 _config.yml 中的其他属性 12345title: Zhangzhiwei's Blog...theme: hexo-theme-next...scheme: Mist 4. 编写更新博客 创建博客 1hexo new '第一个博客' cat source/_posts/第一个博客.md 123456title: 第一个博客date: 2019-09-19 16:58:01tags: - hexocategories: - hexo学习 github 创建一个Repository仓库 1231. 仓库名字必须是 xxx.github.io2. 在settings中 勾选Template repository3. 记得添加自己的ssh github配置 12 # 安装 hexo 关于 git 的组件npm install hexo-deployer-git --save 在_config.yml 中为 git 添加配置 1234deploy: type: git repository: git@github.com:*/*.github.io.git branch: master 查看是否能提交代码github 1ssh -T -ai ~/.ssh/id_rsa git@github.com 部署 1234hexo ghexo d或者hexo d -g 5. next6让首页文字预览显示 5.1 方法一: 自动形成摘要,默认截取的长度为 150 字符 1231. 找到主题的配置文件(themes/next/_config.yml)2. 修改auto_excerpt,把enable改为对应的false改为true3. hexo d -g 5.2 方法二: 博客内容中添加 &lt; !–more–&gt; 123# 安装nodebrew install node npm &lt;!-- more --&gt; 5.3 方法三: 在文章中的front-matter中添加description，并提供文章摘录,这种方式只会在首页列表中显示文章的摘要内容，进入文章详情后不会再显示。 1234567891011title: 部署elk7.2.0date: 2019-09-19 17:59:53copyright: truetags: - k8s - elk - elk7categories: - 技术文档 - elkdescription: 本文主要是简单单机版部署elk7体验, 并非高可用集群方式部署, 部分安装步骤省略. 主要是记录yml配置文件, 仅供参考. 详细内容请点击下方阅读全文, 非常感谢! 6. next6添加搜索功能123456789101. npm install hexo-generator-searchdb --save2. 全局配置文件_config.ymlsearch: path: search.xml field: post format: html limit: 100003. 修改主题的_config.ymllocal_search: enable: true 7. next6 Mist字体的 首页文章间距和首页页宽,字体 7.1 首页文章间距 12345增加一些内容: source/css/_schemes/Mist/_posts-expanded.styl.posts-expand .post &#123; margin-top: 30px; margin-bottom: 30px;&#125; 7.2 页宽 1234source/css/_variables/base.styl$content-desktop = 900px$content-desktop-large = 1000px$content-desktop-largest = 1100px 7.3 字体大小 1234567891011themes/next/source/css/_variables/base.styl$font-size-base = 0.95em;$font-size-base = unit(hexo-config('font.global.size'), em) if hexo-config('font.global.size') is a 'unit';$font-size-smallest = .75em;$font-size-smaller = .8125em;$font-size-small = .855em;$font-size-medium = 0.95em;$font-size-large = 0.975em;$font-size-larger = 1.em;$font-size-largest = 1.125em; 8. 添加网格 8.1 自定义方式修改 1234567891011121314# 新创建自定义文件cat themes/next/source/css/_custom/custom.styl// 主页文章添加阴影效果.post &#123;margin-top: 60px;margin-bottom: 60px;padding: 25px;-webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);-moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);&#125;# 修改config文件vim ./themes/next/_config.ymlcustom: custom 8.2 next6版本修改方式 参考: hexo6–next美化整理 修改 themes/next/layout/_layout.swig 1234&#123;% if theme.canvas_nest %&#125;&lt;script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"&gt;&lt;/script&gt;&#123;% endif %&#125; 将上述代码防止在&lt; /body&gt; 前就可以了(注意不要放在&lt; /head&gt;的后面)。 修改主题的_config.yml 123456canvas_nest: true//color: 线条颜色, 默认: '0,0,0'；三个数字分别为(R,G,B)//opacity: 线条透明度（0~1）, 默认: 0.5//count: 线条的总数量, 默认: 150//zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 注意:我这里打开提示缺少 canvas-nest.min.js文件,这里是手动copy的一份写到 source/lib/canvas-nest/canvas-nest.min.js 1!function()&#123;function o(w,v,i)&#123;return w.getAttribute(v)||i&#125;function j(i)&#123;return document.getElementsByTagName(i)&#125;function l()&#123;var i=j("script"),w=i.length,v=i[w-1];return&#123;l:w,z:o(v,"zIndex",-1),o:o(v,"opacity",0.5),c:o(v,"color","0,0,0"),n:o(v,"count",99)&#125;&#125;function k()&#123;r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight&#125;function b()&#123;e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i)&#123;i.x+=i.xa,i.y+=i.ya,i.xa*=i.x&gt;r||i.x&lt;0?-1:1,i.ya*=i.y&gt;n||i.y&lt;0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v&lt;w.length;v++)&#123;x=w[v];if(i!==x&amp;&amp;null!==x.x&amp;&amp;null!==x.y)&#123;B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y&lt;x.max&amp;&amp;(x===f&amp;&amp;y&gt;=x.max/2&amp;&amp;(i.x-=0.03*B,i.y-=0.03*z),A=(x.max-y)/x.max,e.beginPath(),e.lineWidth=A/2,e.strokeStyle="rgba("+s.c+","+(A+0.2)+")",e.moveTo(i.x,i.y),e.lineTo(x.x,x.y),e.stroke())&#125;&#125;w.splice(w.indexOf(i),1)&#125;),m(b)&#125;var u=document.createElement("canvas"),s=l(),c="c_n"+s.l,e=u.getContext("2d"),r,n,m=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(i)&#123;window.setTimeout(i,1000/45)&#125;,a=Math.random,f=&#123;x:null,y:null,max:20000&#125;;u.id=c;u.style.cssText="position:fixed;top:0;left:0;z-index:"+s.z+";opacity:"+s.o;j("body")[0].appendChild(u);k(),window.onresize=k;window.onmousemove=function(i)&#123;i=i||window.event,f.x=i.clientX,f.y=i.clientY&#125;,window.onmouseout=function()&#123;f.x=null,f.y=null&#125;;for(var t=[],p=0;s.n&gt;p;p++)&#123;var h=a()*r,g=a()*n,q=2*a()-1,d=2*a()-1;t.push(&#123;x:h,y:g,xa:q,ya:d,max:6000&#125;)&#125;setTimeout(function()&#123;b()&#125;,100)&#125;();% 9. 添加评论功能 9.1 注册leancloud 1注册-&gt; 验证邮箱-&gt; 实名认证 -&gt; 设置获取appid和appkey 9.2 修改配置文件 123456valine: enable: true appid: 'appid' appkey: 'appkey' placeholder: "ヾﾉ≧∀≦)o 来呀！快活呀！~啦啦啦~ 啦啦啦啦~" visitor: true //这个打开页会统计文章阅读数 10. next6添加字数统计 和阅读时长 hexo-symbols-count-time 10.1 安装node扩展 1npm install hexo-symbols-count-time --save 10.2 修改全局配置 _config.yml 123456symbols_count_time: symbols: true time: true total_symbols: true total_time: true exclude_codeblock: false 10.3 修改主题配置 _config.yml 1234567symbols_count_time: separated_meta: true item_text_post: true item_text_total: false awl: 4 wpm: 275 suffix: mins. 11. next6 文章置顶功能 11.1 安装node扩展 12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save 11.2 在文章开头添加置顶标识 1top: 10 11.3 首页添加明显置顶标识 123456themes/next/layout/_macro/post.swig 在&lt;div class="post-meta"&gt; 下添加如下代码&#123;% if post.top %&#125; &lt;i class="fa fa-thumb-tack"&gt;&lt;/i&gt; &lt;font color=green&gt;置顶&lt;/font&gt; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125; 12. next6 开启标签和分类 12.1 创建tags相关目录 12hexo new page tagshexo new page categories 12.2 开启tags标签和分类 123vim themes/next/_config.ymltags: /tags/ || tagscategories: /categories/ || th 12.3 修改tags站点文件 12345678cat source/tags/index.md---title: tagsdate: 2019-09-24 10:08:59type: "tags"layout: "tags"comments: false--- 12.4 修改categories站点文件 12345678cat source/categories/index.md---title: categoriesdate: 2019-09-24 10:09:55type: "categories"layout: "categories"comments: false--- 12.5 去掉xxx.github.io/tags/ 页面的post-title(因为我的这个css左对齐了,默认是居中,所以很丑) 123# 注释下面这段代码vim themes/nextv/layout/page.swig &lt;!-- &#123;% include '_partials/page/page-header.swig' %&#125; --&gt; 12.6 文章中多个tag和categories 123456tags: - k8s - k8s安装categories: - [k8s,安装] - [技术文档]]]></content>
      <categories>
        <category>有趣</category>
        <category>博客</category>
        <category>美化</category>
      </categories>
      <tags>
        <tag>hexo6</tag>
        <tag>hexo美化</tag>
      </tags>
  </entry>
</search>
