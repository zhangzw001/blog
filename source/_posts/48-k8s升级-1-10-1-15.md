---
title: k8s升级(1.10->1.15)
copyright: true
date: 2020-05-18 17:43:27
tags:
  - k8s
categories:
  - [k8s]

---

k8s升级一般不能跨版本升级, 所以这里间断介绍升级过程, 每次升级一个大版本
<!--more-->


### k8s升级


[kubernetes集群版本升级攻略](https://blog.51cto.com/newfly/2440901?source=dra)
[kuboard网址k8s升级攻略](https://kuboard.cn/install/upgrade-k8s/1.15.x-1.16.x.html)
[官方kubeadm升级文档](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
[官方k8s版本changelog](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG)


### 版本关系
Kubernetes 1.18.0 -->Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09, 19.03
Kubernetes 1.17.0 -->Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09, 19.03
Kubernetes 1.16.0 -->Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09
Kubernetes 1.15.0 -->Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09
Kubernetes 1.14.0 -->Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09
Kubernetes 1.13.0 -->Docker版本1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06
Kubernetes 1.12.0 -->Docker版本1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06
Kubernetes 1.11.* -->Docker版本1.11.2 to 1.13.1 and 17.03.x (ref)
Kubernetes 1.10.* -->Docker版本1.11.2 to 1.13.1 and 17.03.x (ref)

---

<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> k8s1.10 -> k8s1.11 </font>
</center>


### k8s1.10 -> k8s1.11

-  注意我这里是单机,所以没有禁止调度,集群在升级前需要禁止调度:

> kubectl taint node master-01 node-role.kubernetes.io/master="":NoExecute
> kubectl taint node --all node-role.kubernetes.io/master-

-  注意备份etcd

```
export ETCDCTL_API=3
# 备份
etcdctl snapshot save backup-$(date +%Y%m%d_%H).db
# 恢复
etcdctl snapshot restore backup-xxxx.db --data-dir=/data/etcd/data

```

-  更稳妥的方法应该是先 k8s1.10 -> k8s1.10.13(小版本最后一个版本) -> k8s1.11.0



#### kubeadm升级准备
```
# 保存配置
export k8s_old_version=1.10.0
export k8s_version=1.11.0
export kubeadm_config=kubeadmin-${k8s_old_version}-view.conf

kubeadm config view > ${kubeadm_config}

# k8s1.10版本要求cni必须是低于kubernetes-cni-0.6.0-0版本
yum makecache all
version=$(yum list kubeadm --showduplicates | sort -r|grep ${k8s_version}|awk '{print $2}')
echo $version

# 这里执行不会升级kubelet
yum install  kubeadm-${version} --disableexcludes=kubernetes -y


# 查看是否可以更新
kubeadm upgrade plan
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
I0520 09:32:03.264259   30047 feature_gate.go:230] feature gates: &{map[]}
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.10.0
[upgrade/versions] kubeadm version: v1.11.0
[upgrade/versions] WARNING: Couldn't fetch latest stable version from the internet: unable to get URL "https://dl.k8s.io/release/stable.txt": Get https://storage.googleapis.com/kubernetes-release/release/stable.txt: dial tcp 34.64.4.80:443: i/o timeout
[upgrade/versions] WARNING: Falling back to current kubeadm version as latest stable version
[upgrade/versions] Latest version in the v1.10 series: v1.10.13

External components that should be upgraded manually before you upgrade the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT   AVAILABLE
Etcd        3.3.11    3.1.12

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.10.1   v1.10.13

Upgrade to the latest version in the v1.10 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.10.0   v1.10.13
Controller Manager   v1.10.0   v1.10.13
Scheduler            v1.10.0   v1.10.13
Kube Proxy           v1.10.0   v1.10.13
CoreDNS              1.0.6     1.1.3

You can now apply the upgrade by executing the following command:

 kubeadm upgrade apply v1.10.13

_____________________________________________________________________

External components that should be upgraded manually before you upgrade the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT   AVAILABLE
Etcd        3.3.11    3.2.18

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.10.1   v1.11.0

Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.10.0   v1.11.0
Controller Manager   v1.10.0   v1.11.0
Scheduler            v1.10.0   v1.11.0
Kube Proxy           v1.10.0   v1.11.0
CoreDNS              1.0.6     1.1.3

You can now apply the upgrade by executing the following command:

 kubeadm upgrade apply v1.11.0

_____________________________________________________________________


```


#### 执行升级操作
```
kubeadm upgrade apply v${k8s_version}  --config ${kubeadm_config} --dry-run


# 修改下kubeadmin-10-view.conf配置 imageRepository: registry.cn-hangzhou.aliyuncs.com/k8sth
vim ${kubeadm_config}
imageRepository: registry.cn-hangzhou.aliyuncs.com/k8sth


docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-proxy-amd64:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-controller-manager-amd64:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-scheduler-amd64:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-apiserver-amd64:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/coredns:1.1.3


kubeadm upgrade apply v${k8s_version} --config ${kubeadm_config}

```

#### 升级kubelet kubectl

```
yum install -y kubectl-${version} kubelet-${version} --disableexcludes=kubernetes

# 修改下kubelet配置(否则集群会报错)
vim /var/lib/kubelet/kubeadm-flags.env
增加--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1

systemctl daemon-reload
systemctl stop kubelet
systemctl start kubelet
kubectl version
kubelet --version
kubectl get nodes
```

> 这里执行完, 我的pod mysql和redis等服务器并没有被重启


<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> k8s1.11 -> k8s1.12 </font>
</center>


### k8s1.11 -> k8s1.12
```
# 在这之前先停一下指标 metrics-server 和 prometheus-adapter (我这里会导致api-server无法启动成功,找不到metrics.k8s.io/v1beta1 等)
kubectl delete -f /data/k8s-config/prometheus/prometheus-adapter 
kubectl delete -f /data/k8s-config/metrics-server/metrics-server-0.3.2/deploy/1.8+/


# 保存配置
export k8s_old_version=1.11.0
export k8s_version=1.12.0
export kubeadm_config=kubeadmin-${k8s_old_version}-view.conf

kubeadm config view > ${kubeadm_config}
# 或者通过configmap (注意是 ClusterConfiguration)
kubectl get configmap -n kube-system kubeadm-config -o jsonpath={.data.MasterConfiguration} > ${kubeadm_config}


yum makecache all
version=$(yum list kubeadm --showduplicates | sort -r|grep ${k8s_version}|awk '{print $2}')
echo $version

yum install -y kubeadm-${version} --disableexcludes=kubernetes


kubeadm upgrade plan
COMPONENT   CURRENT   AVAILABLE
Etcd        3.3.11    3.2.24

Upgrade to the latest version in the v1.11 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.11.0   v1.12.0
Controller Manager   v1.11.0   v1.12.0
Scheduler            v1.11.0   v1.12.0
Kube Proxy           v1.11.0   v1.12.0
CoreDNS              1.1.3     1.2.2






#修改image地址(之前是registry.cn-hangzhou.aliyuncs.com/k8sth)
vim ${kubeadm_config}
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers


kubeadm upgrade apply v${k8s_version}  --config ${kubeadm_config} --dry-run

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.2
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.2.24

kubeadm upgrade apply v${k8s_version} --config ${kubeadm_config}


yum install -y kubelet-${version} kubectl-${version} --disableexcludes=kubernetes

systemctl daemon-reload
systemctl restart kubelet
kubectl version
kubelet --version
kubectl get nodes

```

<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> k8s1.12 -> k8s1.13 </font>
</center>



### k8s1.12 -> k8s1.13
```


# 保存配置
export k8s_old_version=1.12.0
export k8s_version=1.13.0
export kubeadm_config=kubeadmin-${k8s_old_version}-view.conf

# 通过configmap
kubectl get configmap -n kube-system kubeadm-config -o jsonpath={.data.ClusterConfiguration} > ${kubeadm_config}

yum makecache all
version=$(yum list kubeadm --showduplicates | sort -r|grep ${k8s_version}|awk '{print $2}')
echo $version

yum install -y kubeadm-${version} --disableexcludes=kubernetes

kubeadm upgrade plan
External components that should be upgraded manually before you upgrade the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT   AVAILABLE
Etcd        3.3.11    3.2.24

Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.12.0   v1.13.0
Controller Manager   v1.12.0   v1.13.0
Scheduler            v1.12.0   v1.13.0
Kube Proxy           v1.12.0   v1.13.0
CoreDNS              1.2.2     1.2.6





kubeadm upgrade apply v${k8s_version}  --config ${kubeadm_config} --dry-run

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6

kubeadm upgrade apply v${k8s_version} --config ${kubeadm_config}

yum install -y kubelet-${version} kubectl-${version} --disableexcludes=kubernetes


systemctl daemon-reload
systemctl restart kubelet
kubectl version
kubelet --version
kubectl get nodes
```


<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> k8s1.13 -> k8s1.14 </font>
</center>

### k8s1.13 -> k8s1.14
```
# 保存配置
export k8s_old_version=1.13.0
export k8s_version=1.14.0
export kubeadm_config=kubeadmin-${k8s_old_version}-view.conf

# config view或者通过configmap(2选1)
kubeadm config view > ${kubeadm_config}
kubectl get configmap -n kube-system kubeadm-config -o jsonpath={.data.ClusterConfiguration} > ${kubeadm_config}


yum makecache all
version=$(yum list kubeadm --showduplicates | sort -r|grep ${k8s_version}|awk '{print $2}')
echo $version


# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) 
# (这里1.14会依赖kubernetes-cni:0.7.5-0)
yum install -y kubeadm-${version} kubelet-${version} kubectl-${version}  --disableexcludes=kubernetes


kubeadm upgrade plan


kubeadm upgrade apply v${k8s_version}  --config ${kubeadm_config} --dry-run

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1


kubeadm upgrade apply v${k8s_version} --config ${kubeadm_config}


systemctl daemon-reload
systemctl restart kubelet
kubectl version
kubelet --version
kubectl get nodes
```


<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> k8s1.14 -> k8s1.15 </font>
</center>



### k8s1.14 -> k8s1.15
```
# 保存配置
export k8s_old_version=1.14.0
export k8s_version=1.15.0
export kubeadm_config=kubeadmin-${k8s_old_version}-view.conf

# config view或者通过configmap(2选1)
kubeadm config view > ${kubeadm_config}
kubectl get configmap -n kube-system kubeadm-config -o jsonpath={.data.ClusterConfiguration} > ${kubeadm_config}


yum makecache all
version=$(yum list kubeadm --showduplicates | sort -r|grep ${k8s_version}|awk '{print $2}')

# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) 
yum install -y kubeadm-${version} kubelet-${version} kubectl-${version}  --disableexcludes=kubernetes

kubeadm upgrade plan

kubeadm upgrade apply v${k8s_version}  --config ${kubeadm_config} --dry-run

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1


kubeadm upgrade apply v${k8s_version} --config ${kubeadm_config}


systemctl daemon-reload
systemctl restart kubelet
kubectl version
kubelet --version
kubectl get nodes
```



<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> k8s1.15.0 -> k8s1.15.11 </font>
</center>



### k8s1.15.0 -> k8s1.15.11 (选择11是因为google_containers/kube-proxy:v1.15.12 not found)
```
# 保存配置
export k8s_old_version=1.15.0
export k8s_version=1.15.11
export kubeadm_config=kubeadmin-${k8s_old_version}-view.conf

# config view或者通过configmap(2选1)
kubeadm config view > ${kubeadm_config}
kubectl get configmap -n kube-system kubeadm-config -o jsonpath={.data.ClusterConfiguration} > ${kubeadm_config}


yum makecache all
version=$(yum list kubeadm --showduplicates | sort -r|grep ${k8s_version}|awk '{print $2}')

# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) 
yum install -y kubeadm-${version}  --disableexcludes=kubernetes

kubeadm upgrade plan
COMPONENT   CURRENT   AVAILABLE
Etcd        3.3.11    3.3.10

COMPONENT            CURRENT   AVAILABLE
API Server           v1.15.0   v1.15.11
Controller Manager   v1.15.0   v1.15.11
Scheduler            v1.15.0   v1.15.11
Kube Proxy           v1.15.0   v1.15.11
CoreDNS              1.3.1     1.3.1

kubeadm upgrade apply v${k8s_version}  --config ${kubeadm_config} --dry-run

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v${k8s_version}
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1


kubeadm upgrade apply v${k8s_version} --config ${kubeadm_config}

yum install -y kubelet-${version} kubectl-${version}  --disableexcludes=kubernetes

systemctl daemon-reload
systemctl restart kubelet
kubectl version
kubelet --version
kubectl get nodes
```



<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> 升级docker </font>
</center>


### 升级docker (慎用)
```
yum install docker-ce-18.09.9 docker-ce-cli-18.09.9

# 重启docker, k8s集群会间断(如果是多台, 将重启的节点taint剔除集群重启即可)
service docker restart

```

---


<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> 升级单机版rancher </font>
</center>

### 升级单机版rancher
>  这里之前是2.1.9版本, 更新为stable(2.4.3) ,我这里rancher只是用导入方式,rancher宕机不影响k8s集群

1. 首先将swarm启动的rancher停用:
 docker service rm rancher_rancher
2. 其次将data目录备份 
 cp -ra /data/rancher /data/rancher_2.1.9_2020-05-20_bak
3. 修改docker-compose.yml重新启动
 docker stack deploy -c docker-compose.yml rancher
4. 查看日志
 docker logs -f $(docker ps|grep rancher_rancher|awk '{print $1}')
5. 恢复(停止服务,将备份的目录还原重新启动即可)

- docker-compose.yml 
```
version: '3'
services:
  rancher:
    hostname: rancher
    image: rancher/rancher:stable
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    volumes:
      - /data/rancher/data/:/var/lib/rancher/
    ports:
      - 10080:80
      - 10443:443
    networks:
      - rancher_net
networks:
  rancher_net:
```



<center>
<img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
<font color="blue" face="黑体" size=5> 问题 </font>
</center>

---



### 问题
#### 1. 版本选择的image配置问题
```
开始选择升级到k8s1.11.10版本, 但是镜像版本只有registry.cn-hangzhou.aliyuncs.com/k8sth/kube-proxy-amd64:v1.11.0 , 没有找到v1.11.10
```

#### 2. 1.12版本开始阿里云镜像地址变化
```
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.12.0
registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1

```

#### 3. 如果执行升级中断或停止, 可以
```
kubeadm upgrade --force。
```

#### 4. 如果跨版本升级
```
 - Specified version to upgrade to "v1.16.2" is too high; kubeadm can upgrade only 1 minor version at a time
 - Specified version to upgrade to "v1.16.2" is at least one minor release higher than the kubeadm minor release (16 > 11). Such an upgrade is not supported
```

#### 5. k8s1.11 开启使用了pause:3.1
```
k8s1.11开始kubelet的配置文件修改为:
vim /var/lib/kubelet/kubeadm-flags.env
增加--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1

```

#### 6. 在1.12->1.13升级完成后报错: OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: OpenAPI spec does not exists
```
# 1.11 -> 1.12 k8s_kube-apiserver日志
E0520 06:21:09.369346       1 memcache.go:134] couldn't get resource list for crd.projectcalico.org/v1: the server could not find the requested resource
E0520 06:21:09.369725       1 memcache.go:134] couldn't get resource list for authentication.istio.io/v1alpha1: the server could not find the requested resource
E0520 06:21:09.370842       1 memcache.go:134] couldn't get resource list for certmanager.k8s.io/v1alpha1: the server could not find the requested resource
E0520 06:21:09.371925       1 memcache.go:134] couldn't get resource list for rbac.istio.io/v1alpha1: the server could not find the requested resource
E0520 06:21:09.373016       1 memcache.go:134] couldn't get resource list for config.istio.io/v1alpha2: the server could not find the requested resource
E0520 06:21:09.374083       1 memcache.go:134] couldn't get resource list for networking.istio.io/v1alpha3: the server could not find the requested resource
E0520 06:21:09.375179       1 memcache.go:134] couldn't get resource list for metrics.k8s.io/v1beta1: the server could not find the requested resource

# 1.12->1.13 k8s_kube-apiserver日志
OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: OpenAPI spec does not exists

原因是我这边metrics-server 和prometheus-adapter 部署的时候用到了metrics.k8s.io/v1beta1
但是升级之后这个api-version丢失了, 所以导致apiserver一直报错, 无法启动成功


```

#### 7. 单机k8s升级会造成服务中断
```
# 1.10 ~ 1.13
升级会导致核心组件apiserver,scheduler,controller,coredns,网络calico等由于版本升级都需要重新启动了一次, 部分其他业务pod也会重启
其中nodejs服务会pm2重启,mysql等也会重启 ,会导致服务器压力上升
这里检测到有状态statefulset的服务并没有重启, 而无状态的deployment会重启, 由于单机1个pod的服务重启会导致中断
所以避免业务中断必须是多主机集群, 在升级之前先taint机器升级

# 1.13 -> 1.14
这期间有状态statefulset的服务也重启了

# 1.14 ~ 1.15.11
这期间有状态statefulset的服务没有重启

```

#### 8. 注意1.13->1.14的时候安装kubeadm可能会依赖kubelet自动安装了最新版本
```
# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) 
# (这里1.14会依赖kubernetes-cni:0.7.5-0)
yum install -y kubeadm-${version} kubelet-${version} kubectl-${version}  --disableexcludes=kubernetes

```
